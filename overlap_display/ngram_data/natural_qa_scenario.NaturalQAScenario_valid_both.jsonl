{"instance": {"input": "Passage: The projected active duty end strength in the armed forces for fiscal year 2017 was 1,281,900 servicemembers,[4] with an additional 801,200 people in the seven reserve components.[4] It is an all-volunteer military, but conscription through the Selective Service System can be enacted at the President's request and Congress' approval. All males ages 18 through 25 who are living in the United States are required to register with the Selective Service for a potential future draft.\n\nQuestion: How many us troops does the united states have?", "references": ["1,281,900 servicemembers,[4] with an additional 801,200 people in the seven reserve components"], "id": "id3660"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "openbook_longans"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3660", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["passage", 1], ["the", 30], ["projected", 30], ["active", 30], ["duty", 30], ["end", 30], ["strength", 30], ["in", 30], ["the", 16], ["armed", 16], ["forces", 16], ["for", 16], ["fiscal", 16], ["year", 16], ["2017", 16], ["was", 16], ["1", 16], ["281", 16], ["900", 16], ["servicemembers", 16], ["4", 17], ["with", 17], ["an", 17], ["additional", 17], ["801", 17], ["200", 17], ["people", 17], ["in", 17], ["the", 17], ["seven", 17], ["reserve", 17], ["components", 17], ["4", 17], ["it", 32], ["is", 32], ["an", 32], ["all", 32], ["volunteer", 31], ["military", 31], ["but", 31], ["conscription", 31], ["through", 31], ["the", 31], ["selective", 31], ["service", 31], ["system", 31], ["can", 31], ["be", 31], ["enacted", 31], ["at", 31], ["the", 31], ["president", 31], ["s", 31], ["request", 31], ["and", 31], ["congress", 31], ["approval", 31], ["all", 32], ["males", 31], ["ages", 31], ["18", 31], ["through", 31], ["25", 31], ["who", 31], ["are", 31], ["living", 31], ["in", 31], ["the", 31], ["united", 31], ["states", 31], ["are", 32], ["required", 0], ["to", 0], ["register", 0], ["with", 0], ["the", 0], ["selective", 0], ["service", 0], ["for", 0], ["a", 0], ["potential", 0], ["future", 0], ["draft", 0], ["question", 0], ["how", 0], ["many", 0], ["us", 0], ["troops", 0], ["does", 0], ["the", 0], ["united", 0], ["states", 0], ["have", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.8658536585365854, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.04838777170978538, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.8829787234042553, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.8829787234042553, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.012195121951219513, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.012195121951219513, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.8829787234042553, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.8829787234042553, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["1", 16], ["281", 16], ["900", 16], ["servicemembers", 16], ["4", 0], ["with", 0], ["an", 0], ["additional", 0], ["801", 0], ["200", 0], ["people", 0], ["in", 0], ["the", 0], ["seven", 0], ["reserve", 0], ["components", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.0625, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.0625, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Passage: Middle C (the fourth C key from left on a standard 88-key piano keyboard) is designated C4 in scientific pitch notation, the most commonly recognized in auditory science[citation needed], while both C4 and the Helmholtz designation c' are used in musical studies. Other note-octave systems, including those used by some makers of digital music keyboards, may refer to Middle C differently. In MIDI, Middle C is note number 60.\n\nQuestion: Which note is middle c on a piano?", "references": ["the fourth C key from left", "the fourth C key from left on a standard 88-key piano keyboard"], "id": "id2474"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "openbook_longans"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id2474", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["passage", 0], ["middle", 1], ["c", 1], ["the", 1], ["fourth", 1], ["c", 1], ["key", 1], ["from", 1], ["left", 1], ["on", 1], ["a", 1], ["standard", 1], ["88", 1], ["key", 1], ["piano", 1], ["keyboard", 1], ["is", 1], ["designated", 1], ["c4", 0], ["in", 0], ["scientific", 0], ["pitch", 0], ["notation", 0], ["the", 0], ["most", 0], ["commonly", 0], ["recognized", 0], ["in", 0], ["auditory", 0], ["science", 0], ["citation", 0], ["needed", 0], ["while", 1], ["both", 1], ["c4", 1], ["and", 1], ["the", 1], ["helmholtz", 1], ["designation", 1], ["c", 1], ["are", 1], ["used", 1], ["in", 1], ["musical", 1], ["studies", 1], ["other", 1], ["note", 1], ["octave", 1], ["systems", 1], ["including", 1], ["those", 1], ["used", 1], ["by", 1], ["some", 1], ["makers", 1], ["of", 1], ["digital", 1], ["music", 1], ["keyboards", 1], ["may", 1], ["refer", 1], ["to", 0], ["middle", 0], ["c", 0], ["differently", 0], ["in", 0], ["midi", 0], ["middle", 0], ["c", 0], ["is", 0], ["note", 0], ["number", 0], ["60", 0], ["question", 0], ["which", 0], ["note", 0], ["is", 0], ["middle", 0], ["c", 0], ["on", 0], ["a", 0], ["piano", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.647887323943662, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.647887323943662, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.8433734939759037, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.8433734939759037, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.647887323943662, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.647887323943662, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.8433734939759037, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.8433734939759037, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["the", 0], ["fourth", 0], ["c", 0], ["key", 0], ["from", 0], ["left", 0], ["the", 1], ["fourth", 0], ["c", 0], ["key", 0], ["from", 0], ["left", 0], ["on", 0], ["a", 0], ["standard", 0], ["88", 0], ["key", 0], ["piano", 0], ["keyboard", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.14285714285714285, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.14285714285714285, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.6842105263157895, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6842105263157895, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.14285714285714285, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.14285714285714285, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.6842105263157895, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6842105263157895, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Passage: The United States Attorney General (A.G.) is the head of the United States Department of Justice per 28 U.S.C. § 503, concerned with all legal affairs, and is the chief lawyer of the United States government. In cases of the federal death penalty, the power to seek the death penalty rests with the Attorney General.\n\nQuestion: What is the role of the us attorney general?", "references": ["the head of the United States Department of Justice per 28 U.S.C. § 503, concerned with all legal affairs, and is the chief lawyer of the United States government", "to prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned, and to give his or her advice and opinion upon questions of law when required by the President of the United States, or when requested by the heads of any of the departments"], "id": "id4040"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "openbook_longans"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id4040", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["passage", 0], ["the", 1], ["united", 1], ["states", 1], ["attorney", 1], ["general", 1], ["a", 0], ["g", 0], ["is", 4], ["the", 4], ["head", 4], ["of", 0], ["the", 0], ["united", 0], ["states", 0], ["department", 0], ["of", 0], ["justice", 0], ["per", 0], ["28", 0], ["u", 0], ["s", 0], ["c", 0], ["§", 0], ["503", 0], ["concerned", 0], ["with", 0], ["all", 0], ["legal", 0], ["affairs", 0], ["and", 0], ["is", 0], ["the", 0], ["chief", 0], ["lawyer", 0], ["of", 0], ["the", 0], ["united", 0], ["states", 0], ["government", 0], ["in", 0], ["cases", 0], ["of", 0], ["the", 0], ["federal", 0], ["death", 0], ["penalty", 0], ["the", 0], ["power", 0], ["to", 0], ["seek", 0], ["the", 0], ["death", 0], ["penalty", 0], ["rests", 0], ["with", 0], ["the", 0], ["attorney", 0], ["general", 0], ["question", 0], ["what", 0], ["is", 0], ["the", 0], ["role", 0], ["of", 0], ["the", 0], ["us", 0], ["attorney", 0], ["general", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.13793103448275862, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.09913793103448276, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.3142857142857143, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.3142857142857143, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.13793103448275862, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.09913793103448276, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.3142857142857143, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.3142857142857143, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["the", 4], ["head", 4], ["of", 0], ["the", 0], ["united", 0], ["states", 0], ["department", 0], ["of", 0], ["justice", 0], ["per", 0], ["28", 0], ["u", 0], ["s", 0], ["c", 0], ["§", 0], ["503", 0], ["concerned", 0], ["with", 0], ["all", 0], ["legal", 0], ["affairs", 0], ["and", 0], ["is", 0], ["the", 0], ["chief", 0], ["lawyer", 0], ["of", 0], ["the", 0], ["united", 0], ["states", 0], ["government", 0], ["to", 5], ["prosecute", 8], ["and", 8], ["conduct", 8], ["all", 8], ["suits", 8], ["in", 8], ["the", 6], ["supreme", 6], ["court", 6], ["in", 0], ["which", 0], ["the", 0], ["united", 0], ["states", 0], ["shall", 0], ["be", 0], ["concerned", 0], ["and", 0], ["to", 0], ["give", 0], ["his", 0], ["or", 0], ["her", 0], ["advice", 8], ["and", 8], ["opinion", 8], ["upon", 8], ["questions", 8], ["of", 8], ["law", 8], ["when", 8], ["required", 8], ["by", 8], ["the", 8], ["president", 7], ["of", 7], ["the", 7], ["united", 6], ["states", 0], ["or", 0], ["when", 0], ["requested", 0], ["by", 0], ["the", 0], ["heads", 0], ["of", 0], ["any", 0], ["of", 0], ["the", 0], ["departments", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.38571428571428573, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.056003401360544214, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.7682926829268293, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.1447590011614401, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.38571428571428573, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.056003401360544214, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7682926829268293, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.1447590011614401, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Passage: The peripheral nervous system (PNS) is one of the two components of the nervous system, the other part is the central nervous system (CNS). The PNS consists of the nerves and ganglia outside the brain and spinal cord.[1] The main function of the PNS is to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body.[2] Unlike the CNS, the PNS is not protected by the vertebral column and skull, or by the blood–brain barrier, which leaves it exposed to toxins and mechanical injuries. The peripheral nervous system is divided into the somatic nervous system and the autonomic nervous system. In the somatic nervous system, the cranial nerves are part of the PNS with the exception of the optic nerve (cranial nerve II), along with the retina. The second cranial nerve is not a true peripheral nerve but a tract of the diencephalon.[3] Cranial nerve ganglia originated in the CNS. However, the remaining ten cranial nerve axons extend beyond the brain and are therefore considered part of the PNS.[4] The autonomic nervous system is an involuntary control of smooth muscle and glands. The connection between CNS and organs allows the system to be in two different functional states: sympathetic and parasympathetic.\n\nQuestion: Explain the function of the peripheral nervous system?", "references": ["to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body"], "id": "id1166"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "openbook_longans"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id1166", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["passage", 0], ["the", 0], ["peripheral", 0], ["nervous", 0], ["system", 0], ["pns", 0], ["is", 0], ["one", 0], ["of", 0], ["the", 0], ["two", 0], ["components", 0], ["of", 0], ["the", 0], ["nervous", 0], ["system", 0], ["the", 0], ["other", 0], ["part", 0], ["is", 0], ["the", 0], ["central", 0], ["nervous", 0], ["system", 0], ["cns", 0], ["the", 0], ["pns", 0], ["consists", 0], ["of", 0], ["the", 0], ["nerves", 0], ["and", 0], ["ganglia", 0], ["outside", 0], ["the", 0], ["brain", 0], ["and", 0], ["spinal", 0], ["cord", 0], ["1", 0], ["the", 2], ["main", 2], ["function", 2], ["of", 2], ["the", 2], ["pns", 2], ["is", 2], ["to", 2], ["connect", 1], ["the", 1], ["cns", 1], ["to", 1], ["the", 1], ["limbs", 1], ["and", 1], ["organs", 1], ["essentially", 1], ["serving", 1], ["as", 1], ["a", 1], ["relay", 2], ["between", 0], ["the", 0], ["brain", 0], ["and", 0], ["spinal", 0], ["cord", 0], ["and", 0], ["the", 0], ["rest", 0], ["of", 0], ["the", 0], ["body", 0], ["2", 0], ["unlike", 0], ["the", 0], ["cns", 0], ["the", 0], ["pns", 0], ["is", 0], ["not", 0], ["protected", 0], ["by", 0], ["the", 0], ["vertebral", 0], ["column", 0], ["and", 1], ["skull", 1], ["or", 1], ["by", 1], ["the", 1], ["blood–brain", 1], ["barrier", 1], ["which", 1], ["leaves", 1], ["it", 0], ["exposed", 0], ["to", 0], ["toxins", 0], ["and", 0], ["mechanical", 0], ["injuries", 0], ["the", 1], ["peripheral", 1], ["nervous", 2], ["system", 2], ["is", 0], ["divided", 1], ["into", 1], ["the", 1], ["somatic", 1], ["nervous", 1], ["system", 1], ["and", 1], ["the", 1], ["autonomic", 1], ["nervous", 1], ["system", 1], ["in", 1], ["the", 1], ["somatic", 1], ["nervous", 1], ["system", 1], ["the", 0], ["cranial", 0], ["nerves", 1], ["are", 0], ["part", 0], ["of", 0], ["the", 0], ["pns", 0], ["with", 0], ["the", 0], ["exception", 0], ["of", 0], ["the", 0], ["optic", 0], ["nerve", 0], ["cranial", 0], ["nerve", 0], ["ii", 0], ["along", 1], ["with", 1], ["the", 1], ["retina", 1], ["the", 1], ["second", 1], ["cranial", 1], ["nerve", 1], ["is", 0], ["not", 0], ["a", 0], ["true", 0], ["peripheral", 0], ["nerve", 0], ["but", 0], ["a", 0], ["tract", 0], ["of", 0], ["the", 0], ["diencephalon", 0], ["3", 0], ["cranial", 0], ["nerve", 0], ["ganglia", 0], ["originated", 0], ["in", 2], ["the", 2], ["cns", 2], ["however", 2], ["the", 2], ["remaining", 2], ["ten", 2], ["cranial", 2], ["nerve", 2], ["axons", 2], ["extend", 0], ["beyond", 0], ["the", 0], ["brain", 0], ["and", 0], ["are", 0], ["therefore", 0], ["considered", 0], ["part", 0], ["of", 0], ["the", 0], ["pns", 0], ["4", 0], ["the", 0], ["autonomic", 0], ["nervous", 0], ["system", 0], ["is", 0], ["an", 0], ["involuntary", 0], ["control", 0], ["of", 0], ["smooth", 0], ["muscle", 0], ["and", 0], ["glands", 0], ["the", 2], ["connection", 2], ["between", 2], ["cns", 2], ["and", 2], ["organs", 2], ["allows", 2], ["the", 0], ["system", 0], ["to", 0], ["be", 0], ["in", 0], ["two", 0], ["different", 0], ["functional", 0], ["states", 0], ["sympathetic", 0], ["and", 0], ["parasympathetic", 0], ["question", 0], ["explain", 0], ["the", 0], ["function", 0], ["of", 0], ["the", 0], ["peripheral", 0], ["nervous", 0], ["system", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.3470319634703196, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.2831050228310502, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.6320346320346321, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.525974025974026, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.3470319634703196, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.2831050228310502, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.6320346320346321, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.525974025974026, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["to", 2], ["connect", 1], ["the", 1], ["cns", 1], ["to", 1], ["the", 1], ["limbs", 1], ["and", 1], ["organs", 1], ["essentially", 1], ["serving", 1], ["as", 1], ["a", 1], ["relay", 2], ["between", 0], ["the", 0], ["brain", 0], ["and", 0], ["spinal", 0], ["cord", 0], ["and", 0], ["the", 0], ["rest", 0], ["of", 0], ["the", 0], ["body", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.9285714285714286, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.9807692307692307, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.9285714285714286, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.9807692307692307, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Passage: A costume party (American English) or a fancy dress party (British English) is a type of party, common mainly in contemporary Western culture, where guests dress up in costumes. Costumed Halloween parties are popular in the United States, Canada, Australia, and New Zealand.\n\n\nQuestion: What does fancy dress mean in the uk?", "references": ["A costume", "a type of party, common mainly in contemporary Western culture, where guests dress up in costumes", "guests dress up in costumes", "costumes"], "id": "id1586"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "openbook_longans"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id1586", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["passage", 0], ["a", 1], ["costume", 1], ["party", 1], ["american", 1], ["english", 1], ["or", 1], ["a", 1], ["fancy", 1], ["dress", 1], ["party", 1], ["british", 1], ["english", 1], ["is", 1], ["a", 1], ["type", 1], ["of", 1], ["party", 1], ["common", 1], ["mainly", 1], ["in", 1], ["contemporary", 1], ["western", 1], ["culture", 1], ["where", 1], ["guests", 1], ["dress", 1], ["up", 1], ["in", 1], ["costumes", 1], ["costumed", 1], ["halloween", 1], ["parties", 0], ["are", 0], ["popular", 0], ["in", 0], ["the", 0], ["united", 0], ["states", 0], ["canada", 0], ["australia", 0], ["and", 0], ["new", 0], ["zealand", 0], ["question", 0], ["what", 0], ["does", 0], ["fancy", 0], ["dress", 0], ["mean", 0], ["in", 0], ["the", 0], ["uk", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.7380952380952381, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.7380952380952381, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.7962962962962963, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.7962962962962963, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.7380952380952381, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.7380952380952381, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7962962962962963, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.7962962962962963, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["a", 0], ["costume", 0], ["a", 1], ["type", 1], ["of", 1], ["party", 1], ["common", 0], ["mainly", 0], ["in", 0], ["contemporary", 0], ["western", 0], ["culture", 0], ["where", 0], ["guests", 0], ["dress", 0], ["up", 0], ["in", 0], ["costumes", 0], ["guests", 0], ["dress", 0], ["up", 0], ["in", 0], ["costumes", 0], ["costumes", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.3333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.3333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.6666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.3333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.3333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.6666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Passage: The three wise monkeys (Japanese: 三猿, Hepburn: san'en or sanzaru, alternatively 三匹の猿 sanbiki no saru, literally \"three monkeys\"), sometimes called the three mystic apes,[1] are a pictorial maxim. Together they embody the proverbial principle \"see no evil, hear no evil, speak no evil\".[2] The three monkeys are Mizaru, covering his eyes, who sees no evil; Kikazaru, covering his ears, who hears no evil; and Iwazaru, covering his mouth, who speaks no evil.[3]\n\nQuestion: Where does see no evil speak no evil come from?", "references": ["a 17th-century carving over a door of the famous Tōshō-gū shrine in Nikkō, Japan", "The three wise monkeys (Japanese: 三猿, Hepburn: san'en or sanzaru, alternatively 三匹の猿 sanbiki no saru, literally \"three monkeys\"), sometimes called the three mystic apes"], "id": "id1882"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "openbook_longans"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id1882", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["passage", 0], ["the", 0], ["three", 0], ["wise", 0], ["monkeys", 0], ["japanese", 0], ["三猿", 0], ["hepburn", 0], ["san", 0], ["en", 0], ["or", 0], ["sanzaru", 0], ["alternatively", 0], ["三匹の猿", 0], ["sanbiki", 0], ["no", 0], ["saru", 0], ["literally", 0], ["three", 0], ["monkeys", 0], ["sometimes", 0], ["called", 0], ["the", 0], ["three", 0], ["mystic", 0], ["apes", 0], ["1", 0], ["are", 1], ["a", 1], ["pictorial", 1], ["maxim", 1], ["together", 2], ["they", 2], ["embody", 3], ["the", 0], ["proverbial", 0], ["principle", 0], ["see", 0], ["no", 0], ["evil", 0], ["hear", 0], ["no", 0], ["evil", 0], ["speak", 0], ["no", 0], ["evil", 0], ["2", 0], ["the", 5], ["three", 5], ["monkeys", 5], ["are", 5], ["mizaru", 5], ["covering", 5], ["his", 5], ["eyes", 5], ["who", 5], ["sees", 5], ["no", 5], ["evil", 5], ["kikazaru", 5], ["covering", 5], ["his", 5], ["ears", 5], ["who", 5], ["hears", 0], ["no", 0], ["evil", 0], ["and", 0], ["iwazaru", 0], ["covering", 0], ["his", 0], ["mouth", 0], ["who", 0], ["speaks", 0], ["no", 0], ["evil", 0], ["3", 0], ["question", 0], ["where", 0], ["does", 0], ["see", 0], ["no", 0], ["evil", 0], ["speak", 0], ["no", 0], ["evil", 0], ["come", 0], ["from", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.3116883116883117, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.1134199134199134, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.5393258426966292, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.2786516853932582, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.3116883116883117, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.1134199134199134, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.5393258426966292, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.2786516853932582, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["a", 1], ["17th", 1], ["century", 1], ["carving", 1], ["over", 0], ["a", 0], ["door", 0], ["of", 0], ["the", 0], ["famous", 0], ["tōshō", 0], ["gū", 0], ["shrine", 0], ["in", 0], ["nikkō", 0], ["japan", 0], ["the", 0], ["three", 0], ["wise", 0], ["monkeys", 0], ["japanese", 0], ["三猿", 0], ["hepburn", 0], ["san", 0], ["en", 0], ["or", 0], ["sanzaru", 0], ["alternatively", 0], ["三匹の猿", 0], ["sanbiki", 0], ["no", 0], ["saru", 0], ["literally", 0], ["three", 0], ["monkeys", 0], ["sometimes", 0], ["called", 0], ["the", 0], ["three", 0], ["mystic", 0], ["apes", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.13793103448275862, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.13793103448275862, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.3902439024390244, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.3902439024390244, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.13793103448275862, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.13793103448275862, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.3902439024390244, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.3902439024390244, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Passage: Ashes are ceremonially placed on the heads of Christians on Ash Wednesday, either by being sprinkled over their heads or, in English-speaking countries, more often by being marked on their foreheads as a visible cross. The words (based on Genesis 3:19) used traditionally to accompany this gesture are, \"Memento, homo, quia pulvis es, et in pulverem reverteris.\" (\"Remember, man, that thou art dust, and to dust thou shalt return.\") This custom is credited to Pope Gregory I the Great (c. 540–604).[26] In the 1969 revision of the Roman Rite, an alternative formula (based on Mark 1:15) was introduced and given first place \"Repent, and believe in the Gospel\" and the older formula was translated as \"Remember that you are dust, and to dust you shall return.\" The old formula, based on the words spoken to Adam and Eve after their sin,[27] reminds worshippers of their sinfulness and mortality and thus, implicitly, of their need to repent in time.[28] The newer formula makes explicit what was only implicit in the old.\n\nQuestion: What's with the ashes on ash wednesday?", "references": ["reminds worshippers of their sinfulness and mortality and thus, implicitly, of their need to repent in time"], "id": "id3504"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "openbook_longans"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3504", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["passage", 0], ["ashes", 2], ["are", 2], ["ceremonially", 2], ["placed", 2], ["on", 2], ["the", 2], ["heads", 2], ["of", 2], ["christians", 2], ["on", 2], ["ash", 2], ["wednesday", 2], ["either", 2], ["by", 2], ["being", 2], ["sprinkled", 2], ["over", 2], ["their", 2], ["heads", 2], ["or", 2], ["in", 2], ["english", 2], ["speaking", 2], ["countries", 2], ["more", 2], ["often", 2], ["by", 2], ["being", 2], ["marked", 2], ["on", 2], ["their", 2], ["foreheads", 2], ["as", 2], ["a", 2], ["visible", 2], ["cross", 2], ["the", 2], ["words", 2], ["based", 0], ["on", 0], ["genesis", 0], ["3", 0], ["19", 0], ["used", 0], ["traditionally", 0], ["to", 0], ["accompany", 0], ["this", 0], ["gesture", 0], ["are", 0], ["memento", 0], ["homo", 0], ["quia", 0], ["pulvis", 0], ["es", 0], ["et", 0], ["in", 0], ["pulverem", 0], ["reverteris", 0], ["remember", 0], ["man", 0], ["that", 0], ["thou", 0], ["art", 0], ["dust", 0], ["and", 0], ["to", 0], ["dust", 0], ["thou", 0], ["shalt", 0], ["return", 0], ["this", 0], ["custom", 0], ["is", 0], ["credited", 0], ["to", 0], ["pope", 0], ["gregory", 0], ["i", 0], ["the", 0], ["great", 0], ["c", 0], ["540–604", 0], ["26", 0], ["in", 2], ["the", 2], ["1969", 2], ["revision", 2], ["of", 2], ["the", 2], ["roman", 2], ["rite", 2], ["an", 2], ["alternative", 2], ["formula", 2], ["based", 2], ["on", 2], ["mark", 2], ["1", 2], ["15", 2], ["was", 0], ["introduced", 0], ["and", 0], ["given", 0], ["first", 0], ["place", 0], ["repent", 0], ["and", 0], ["believe", 0], ["in", 0], ["the", 0], ["gospel", 0], ["and", 0], ["the", 0], ["older", 0], ["formula", 0], ["was", 0], ["translated", 0], ["as", 0], ["remember", 0], ["that", 0], ["you", 0], ["are", 0], ["dust", 0], ["and", 0], ["to", 0], ["dust", 0], ["you", 0], ["shall", 0], ["return", 0], ["the", 2], ["old", 2], ["formula", 2], ["based", 0], ["on", 0], ["the", 0], ["words", 0], ["spoken", 0], ["to", 0], ["adam", 0], ["and", 0], ["eve", 0], ["after", 0], ["their", 0], ["sin", 0], ["27", 0], ["reminds", 2], ["worshippers", 2], ["of", 2], ["their", 2], ["sinfulness", 2], ["and", 0], ["mortality", 0], ["and", 0], ["thus", 0], ["implicitly", 0], ["of", 0], ["their", 0], ["need", 0], ["to", 0], ["repent", 0], ["in", 0], ["time", 0], ["28", 0], ["the", 0], ["newer", 0], ["formula", 0], ["makes", 0], ["explicit", 0], ["what", 0], ["was", 0], ["only", 0], ["implicit", 0], ["in", 0], ["the", 0], ["old", 0], ["question", 0], ["what", 0], ["s", 0], ["with", 0], ["the", 0], ["ashes", 0], ["on", 0], ["ash", 0], ["wednesday", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.35428571428571426, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.17714285714285713, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.5882352941176471, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.29411764705882354, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.35428571428571426, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.17714285714285713, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.5882352941176471, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.29411764705882354, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["reminds", 2], ["worshippers", 2], ["of", 2], ["their", 2], ["sinfulness", 2], ["and", 0], ["mortality", 0], ["and", 0], ["thus", 0], ["implicitly", 0], ["of", 0], ["their", 0], ["need", 0], ["to", 0], ["repent", 0], ["in", 0], ["time", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Passage: Kickstarter is one of a number of crowdfunding platforms for gathering money from the public, which circumvents traditional avenues of investment.[25][26] Project creators choose a deadline and a minimum funding goal. If the goal is not met by the deadline, no funds are collected (a kind of assurance contract).[27] The platform is open to backers from anywhere in the world and to creators from the US, UK,[28] Canada,[29] Australia, New Zealand,[20] The Netherlands, Denmark, Ireland, Norway, Sweden, Spain, France, Germany, Austria, Italy, Belgium, Luxembourg, Switzerland and Mexico.\n\nQuestion: What is the purpose of a website kick starter?", "references": ["global crowdfunding platform focused on creativity and merchandising", "crowdfunding platforms for gathering money from the public, which circumvents traditional avenues of investment"], "id": "id2332"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "openbook_longans"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id2332", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["passage", 0], ["kickstarter", 3], ["is", 4], ["one", 4], ["of", 3], ["a", 3], ["number", 3], ["of", 3], ["crowdfunding", 3], ["platforms", 3], ["for", 2], ["gathering", 2], ["money", 2], ["from", 2], ["the", 2], ["public", 2], ["which", 2], ["circumvents", 2], ["traditional", 2], ["avenues", 2], ["of", 2], ["investment", 2], ["25", 2], ["26", 2], ["project", 5], ["creators", 5], ["choose", 5], ["a", 5], ["deadline", 5], ["and", 5], ["a", 5], ["minimum", 5], ["funding", 5], ["goal", 5], ["if", 5], ["the", 3], ["goal", 3], ["is", 3], ["not", 3], ["met", 3], ["by", 2], ["the", 2], ["deadline", 2], ["no", 2], ["funds", 2], ["are", 2], ["collected", 2], ["a", 2], ["kind", 2], ["of", 2], ["assurance", 2], ["contract", 2], ["27", 2], ["the", 3], ["platform", 3], ["is", 3], ["open", 3], ["to", 3], ["backers", 3], ["from", 2], ["anywhere", 2], ["in", 2], ["the", 2], ["world", 2], ["and", 2], ["to", 2], ["creators", 2], ["from", 2], ["the", 2], ["us", 2], ["uk", 2], ["28", 2], ["canada", 2], ["29", 2], ["australia", 2], ["new", 2], ["zealand", 2], ["20", 2], ["the", 2], ["netherlands", 2], ["denmark", 2], ["ireland", 2], ["norway", 0], ["sweden", 0], ["spain", 0], ["france", 0], ["germany", 0], ["austria", 0], ["italy", 0], ["belgium", 0], ["luxembourg", 0], ["switzerland", 0], ["and", 0], ["mexico", 0], ["question", 0], ["what", 0], ["is", 0], ["the", 0], ["purpose", 0], ["of", 0], ["a", 0], ["website", 0], ["kick", 0], ["starter", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.8709677419354839, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.36236559139784935, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.8857142857142857, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.42857142857142855, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.8709677419354839, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.36236559139784935, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.8857142857142857, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.42857142857142855, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["global", 0], ["crowdfunding", 0], ["platform", 0], ["focused", 0], ["on", 0], ["creativity", 0], ["and", 0], ["merchandising", 0], ["crowdfunding", 3], ["platforms", 3], ["for", 0], ["gathering", 0], ["money", 0], ["from", 0], ["the", 0], ["public", 0], ["which", 0], ["circumvents", 0], ["traditional", 0], ["avenues", 0], ["of", 0], ["investment", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.2, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.06666666666666667, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.6363636363636364, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.2121212121212121, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.2, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.06666666666666667, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.6363636363636364, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.2121212121212121, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Passage: The Gulf of Mexico formed approximately 300 million years ago as a result of plate tectonics.[3] The Gulf of Mexico basin is roughly oval and is approximately 810 nautical miles (1,500 km; 930 mi) wide and floored by sedimentary rocks and recent sediments. It is connected to part of the Atlantic Ocean through the Florida Straits between the U.S. and Cuba, and with the Caribbean (with which it forms the American Mediterranean Sea) via the Yucatán Channel between Mexico and Cuba. With the narrow connection to the Atlantic, the Gulf experiences very small tidal ranges. The size of the Gulf basin is approximately 1.6 million km2 (615,000 sq mi). Almost half of the basin is shallow continental shelf waters. The basin contains a volume of roughly 2,500 quadrillion liters (550 quadrillion Imperial gallons, 660 quadrillion US gallons, 2.5 million km3 or 600,000 cu mi).[4]\n\nQuestion: How many gallons of water in the gulf of mexico?", "references": ["2,500 quadrillion liters (550 quadrillion Imperial gallons, 660 quadrillion US gallons, 2.5 million km3 or 600,000 cu mi)", "660 quadrillion US gallons"], "id": "id3734"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "openbook_longans"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3734", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["passage", 0], ["the", 17], ["gulf", 17], ["of", 17], ["mexico", 17], ["formed", 1], ["approximately", 1], ["300", 1], ["million", 1], ["years", 1], ["ago", 1], ["as", 1], ["a", 1], ["result", 1], ["of", 1], ["plate", 1], ["tectonics", 1], ["3", 0], ["the", 0], ["gulf", 0], ["of", 0], ["mexico", 0], ["basin", 0], ["is", 0], ["roughly", 0], ["oval", 0], ["and", 0], ["is", 0], ["approximately", 0], ["810", 0], ["nautical", 0], ["miles", 0], ["1", 0], ["500", 0], ["km", 0], ["930", 0], ["mi", 0], ["wide", 17], ["and", 17], ["floored", 17], ["by", 17], ["sedimentary", 17], ["rocks", 17], ["and", 17], ["recent", 17], ["sediments", 17], ["it", 17], ["is", 17], ["connected", 17], ["to", 17], ["part", 17], ["of", 17], ["the", 17], ["atlantic", 17], ["ocean", 17], ["through", 17], ["the", 18], ["florida", 1], ["straits", 1], ["between", 1], ["the", 1], ["u", 1], ["s", 1], ["and", 1], ["cuba", 1], ["and", 1], ["with", 1], ["the", 1], ["caribbean", 1], ["with", 1], ["which", 1], ["it", 1], ["forms", 1], ["the", 1], ["american", 1], ["mediterranean", 1], ["sea", 1], ["via", 1], ["the", 1], ["yucatán", 1], ["channel", 17], ["between", 17], ["mexico", 17], ["and", 17], ["cuba", 17], ["with", 17], ["the", 17], ["narrow", 17], ["connection", 17], ["to", 17], ["the", 17], ["atlantic", 17], ["the", 17], ["gulf", 17], ["experiences", 17], ["very", 17], ["small", 17], ["tidal", 17], ["ranges", 17], ["the", 8], ["size", 1], ["of", 1], ["the", 1], ["gulf", 1], ["basin", 1], ["is", 1], ["approximately", 1], ["1", 1], ["6", 1], ["million", 1], ["km2", 1], ["615", 1], ["000", 1], ["sq", 1], ["mi", 1], ["almost", 17], ["half", 17], ["of", 17], ["the", 17], ["basin", 17], ["is", 10], ["shallow", 10], ["continental", 10], ["shelf", 10], ["waters", 1], ["the", 1], ["basin", 1], ["contains", 1], ["a", 1], ["volume", 1], ["of", 1], ["roughly", 1], ["2", 1], ["500", 1], ["quadrillion", 1], ["liters", 1], ["550", 1], ["quadrillion", 1], ["imperial", 1], ["gallons", 1], ["660", 1], ["quadrillion", 1], ["us", 0], ["gallons", 0], ["2", 0], ["5", 0], ["million", 0], ["km3", 0], ["or", 0], ["600", 0], ["000", 0], ["cu", 0], ["mi", 0], ["4", 0], ["question", 0], ["how", 0], ["many", 0], ["gallons", 0], ["of", 0], ["water", 0], ["in", 0], ["the", 0], ["gulf", 0], ["of", 0], ["mexico", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.7857142857142857, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.46328091842797775, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.8734939759036144, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.7374202693125442, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.474025974025974, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.4449675324675325, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7289156626506024, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.7289156626506024, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["2", 1], ["500", 1], ["quadrillion", 1], ["liters", 1], ["550", 1], ["quadrillion", 1], ["imperial", 1], ["gallons", 1], ["660", 1], ["quadrillion", 0], ["us", 0], ["gallons", 0], ["2", 0], ["5", 0], ["million", 0], ["km3", 0], ["or", 0], ["600", 0], ["000", 0], ["cu", 0], ["mi", 0], ["660", 0], ["quadrillion", 0], ["us", 0], ["gallons", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6923076923076923, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6923076923076923, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.84, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.84, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6923076923076923, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6923076923076923, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.84, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.84, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Passage: In the New Thought philosophy , the law of attraction is the belief that by focusing on positive or negative thoughts people can bring positive or negative experiences into their life.[1][2] The belief is based on the idea that people and their thoughts are both made from \"pure energy\", and that through the process of \"like energy attracting like energy\" a person can improve their own health, wealth and personal relationships.\n\nQuestion: What is the secret of the law of attraction?", "references": ["the belief that by focusing on positive or negative thoughts people can bring positive or negative experiences into their life"], "id": "id1360"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "openbook_longans"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id1360", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["passage", 0], ["in", 39], ["the", 40], ["new", 40], ["thought", 40], ["philosophy", 40], ["the", 44], ["law", 44], ["of", 44], ["attraction", 39], ["is", 39], ["the", 40], ["belief", 41], ["that", 41], ["by", 41], ["focusing", 42], ["on", 42], ["positive", 42], ["or", 42], ["negative", 39], ["thoughts", 39], ["people", 39], ["can", 39], ["bring", 39], ["positive", 40], ["or", 40], ["negative", 40], ["experiences", 40], ["into", 40], ["their", 40], ["life", 40], ["1", 40], ["2", 40], ["the", 42], ["belief", 43], ["is", 43], ["based", 43], ["on", 40], ["the", 51], ["idea", 50], ["that", 39], ["people", 39], ["and", 39], ["their", 39], ["thoughts", 39], ["are", 39], ["both", 39], ["made", 40], ["from", 40], ["pure", 40], ["energy", 40], ["and", 40], ["that", 40], ["through", 41], ["the", 41], ["process", 40], ["of", 40], ["like", 40], ["energy", 40], ["attracting", 40], ["like", 40], ["energy", 0], ["a", 0], ["person", 0], ["can", 0], ["improve", 0], ["their", 0], ["own", 0], ["health", 0], ["wealth", 0], ["and", 0], ["personal", 0], ["relationships", 0], ["question", 0], ["what", 0], ["is", 0], ["the", 0], ["secret", 0], ["of", 0], ["the", 0], ["law", 0], ["of", 0], ["attraction", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.8333333333333334, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.02053015731048245, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.8571428571428571, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.02197802197802195, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["the", 40], ["belief", 41], ["that", 41], ["by", 41], ["focusing", 42], ["on", 42], ["positive", 42], ["or", 42], ["negative", 0], ["thoughts", 0], ["people", 0], ["can", 0], ["bring", 0], ["positive", 0], ["or", 0], ["negative", 0], ["experiences", 0], ["into", 0], ["their", 0], ["life", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.02417610336817654, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.025000000000000005, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Passage: Brown v. Board of Education of Topeka, 347 U.S. 483 (1954), was a landmark United States Supreme Court case in which the Court declared state laws establishing separate public schools for black and white students to be unconstitutional. The decision effectively overturned the Plessy v. Ferguson decision of 1896, which allowed state-sponsored segregation, insofar as it applied to public education. Handed down on May 17, 1954, the Warren Court's unanimous (9–0) decision stated that \"separate educational facilities are inherently unequal.\" As a result, de jure racial segregation was ruled a violation of the Equal Protection Clause of the Fourteenth Amendment of the United States Constitution. This ruling paved the way for integration and was a major victory of the Civil Rights Movement,[1] and a model for many future impact litigation cases.[2] However, the decision's fourteen pages did not spell out any sort of method for ending racial segregation in schools, and the Court's second decision in Brown II (349 U.S. 294 (1955)) only ordered states to desegregate \"with all deliberate speed\".\n\nQuestion: What was the outcome of the famous 1954 case of brown v. board of education of topeka?", "references": ["the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "declared state laws establishing separate public schools for black and white students to be unconstitutional"], "id": "id2258"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "openbook_longans"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id2258", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["passage", 0], ["brown", 6], ["v", 6], ["board", 6], ["of", 6], ["education", 6], ["of", 6], ["topeka", 6], ["347", 6], ["u", 6], ["s", 6], ["483", 7], ["1954", 7], ["was", 12], ["a", 11], ["landmark", 12], ["united", 12], ["states", 11], ["supreme", 13], ["court", 13], ["case", 13], ["in", 14], ["which", 14], ["the", 15], ["court", 17], ["declared", 17], ["state", 18], ["laws", 18], ["establishing", 6], ["separate", 6], ["public", 2], ["schools", 2], ["for", 2], ["black", 2], ["and", 2], ["white", 2], ["students", 2], ["to", 2], ["be", 2], ["unconstitutional", 2], ["the", 0], ["decision", 0], ["effectively", 0], ["overturned", 6], ["the", 5], ["plessy", 5], ["v", 5], ["ferguson", 5], ["decision", 6], ["of", 6], ["1896", 6], ["which", 4], ["allowed", 4], ["state", 4], ["sponsored", 4], ["segregation", 4], ["insofar", 4], ["as", 4], ["it", 4], ["applied", 1], ["to", 1], ["public", 1], ["education", 1], ["handed", 1], ["down", 1], ["on", 1], ["may", 1], ["17", 1], ["1954", 1], ["the", 1], ["warren", 1], ["court", 1], ["s", 1], ["unanimous", 1], ["9–0", 1], ["decision", 2], ["stated", 2], ["that", 2], ["separate", 3], ["educational", 2], ["facilities", 2], ["are", 2], ["inherently", 2], ["unequal", 2], ["as", 5], ["a", 5], ["result", 5], ["de", 6], ["jure", 6], ["racial", 9], ["segregation", 10], ["was", 11], ["ruled", 9], ["a", 13], ["violation", 10], ["of", 11], ["the", 33], ["equal", 5], ["protection", 4], ["clause", 4], ["of", 5], ["the", 5], ["fourteenth", 5], ["amendment", 5], ["of", 4], ["the", 4], ["united", 4], ["states", 2], ["constitution", 2], ["this", 2], ["ruling", 2], ["paved", 2], ["the", 2], ["way", 3], ["for", 0], ["integration", 0], ["and", 0], ["was", 0], ["a", 0], ["major", 0], ["victory", 0], ["of", 0], ["the", 0], ["civil", 0], ["rights", 0], ["movement", 0], ["1", 0], ["and", 0], ["a", 0], ["model", 0], ["for", 0], ["many", 0], ["future", 0], ["impact", 0], ["litigation", 0], ["cases", 0], ["2", 0], ["however", 0], ["the", 0], ["decision", 0], ["s", 0], ["fourteen", 1], ["pages", 1], ["did", 2], ["not", 2], ["spell", 2], ["out", 2], ["any", 0], ["sort", 0], ["of", 0], ["method", 0], ["for", 0], ["ending", 0], ["racial", 0], ["segregation", 0], ["in", 0], ["schools", 0], ["and", 0], ["the", 0], ["court", 0], ["s", 0], ["second", 0], ["decision", 0], ["in", 0], ["brown", 0], ["ii", 0], ["349", 0], ["u", 0], ["s", 0], ["294", 0], ["1955", 0], ["only", 0], ["ordered", 0], ["states", 0], ["to", 0], ["desegregate", 0], ["with", 0], ["all", 0], ["deliberate", 0], ["speed", 0], ["question", 0], ["what", 0], ["was", 0], ["the", 0], ["outcome", 0], ["of", 0], ["the", 0], ["famous", 0], ["1954", 0], ["case", 0], ["of", 0], ["brown", 0], ["v", 0], ["board", 0], ["of", 0], ["education", 0], ["of", 0], ["topeka", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6203208556149733, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.2321631917416757, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.7185929648241206, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.52428810720268, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.5187165775401069, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.22473049825991012, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7185929648241206, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.52428810720268, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["the", 15], ["court", 17], ["declared", 17], ["state", 18], ["laws", 18], ["establishing", 0], ["separate", 0], ["public", 0], ["schools", 0], ["for", 0], ["black", 0], ["and", 0], ["white", 0], ["students", 0], ["to", 0], ["be", 0], ["unconstitutional", 0], ["declared", 17], ["state", 18], ["laws", 18], ["establishing", 0], ["separate", 0], ["public", 0], ["schools", 0], ["for", 0], ["black", 0], ["and", 0], ["white", 0], ["students", 0], ["to", 0], ["be", 0], ["unconstitutional", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.4, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.02326797385620915, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.06666666666666667, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Passage: A board of directors is a recognized group of people who jointly oversee the activities of an organization, which can be either a for-profit business, nonprofit organization, or a government agency. Such a board's powers, duties, and responsibilities are determined by government regulations (including the jurisdiction's corporations law) and the organization's own constitution and bylaws. These authorities may specify the number of members of the board, how they are to be chosen, and how often they are to meet.\n\nQuestion: What does the board of directors consist of?", "references": ["a recognized group of people who jointly oversee the activities of an organization"], "id": "id97"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "openbook_longans"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id97", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["passage", 0], ["a", 1], ["board", 1], ["of", 1], ["directors", 1], ["is", 1], ["a", 1], ["recognized", 1], ["group", 1], ["of", 1], ["people", 1], ["who", 1], ["jointly", 1], ["oversee", 1], ["the", 4], ["activities", 4], ["of", 4], ["an", 4], ["organization", 4], ["which", 4], ["can", 4], ["be", 4], ["either", 4], ["a", 4], ["for", 4], ["profit", 4], ["business", 3], ["nonprofit", 3], ["organization", 3], ["or", 3], ["a", 3], ["government", 3], ["agency", 3], ["such", 3], ["a", 3], ["board", 3], ["s", 1], ["powers", 1], ["duties", 1], ["and", 1], ["responsibilities", 1], ["are", 1], ["determined", 1], ["by", 1], ["government", 1], ["regulations", 1], ["including", 1], ["the", 1], ["jurisdiction", 1], ["s", 1], ["corporations", 1], ["law", 1], ["and", 2], ["the", 2], ["organization", 2], ["s", 2], ["own", 2], ["constitution", 5], ["and", 5], ["bylaws", 5], ["these", 5], ["authorities", 5], ["may", 5], ["specify", 8], ["the", 8], ["number", 7], ["of", 4], ["members", 4], ["of", 4], ["the", 4], ["board", 4], ["how", 4], ["they", 0], ["are", 0], ["to", 0], ["be", 0], ["chosen", 0], ["and", 0], ["how", 0], ["often", 0], ["they", 0], ["are", 0], ["to", 0], ["meet", 0], ["question", 0], ["what", 0], ["does", 0], ["the", 0], ["board", 0], ["of", 0], ["directors", 0], ["consist", 0], ["of", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.8658536585365854, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.49909988385598153, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.8829787234042553, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.8829787234042553, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.8658536585365854, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.49909988385598153, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.8829787234042553, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.8829787234042553, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["a", 1], ["recognized", 0], ["group", 0], ["of", 0], ["people", 0], ["who", 0], ["jointly", 0], ["oversee", 0], ["the", 0], ["activities", 0], ["of", 0], ["an", 0], ["organization", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Passage: The Stations of the Cross or the Way of the Cross, also known as the Way of Sorrows or the Via Crucis, refers to a series of images depicting Jesus Christ on the day of his crucifixion and accompanying prayers. The stations grew out of imitations of Via Dolorosa in Jerusalem which is believed to be the actual path Jesus walked to Mount Calvary. The object of the stations is to help the Christians faithful to make a spiritual pilgrimage through contemplation of the Passion of Christ. It has become one of the most popular devotions and the stations can be found in many Western Christian churches, including Anglican,[1] Lutheran,[2] Methodist,[3] and Roman Catholic ones.\n\nQuestion: The origins of the stations of the cross?", "references": ["Via Dolorosa in Jerusalem which is believed to be the actual path Jesus walked to Mount Calvary", "pilgrimages to Jerusalem and a desire to reproduce Via Dolorosa", "grew out of imitations of Via Dolorosa in Jerusalem which is believed to be the actual path Jesus walked to Mount Calvary"], "id": "id936"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "openbook_longans"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id936", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["passage", 0], ["the", 0], ["stations", 1], ["of", 0], ["the", 0], ["cross", 0], ["or", 0], ["the", 0], ["way", 0], ["of", 0], ["the", 0], ["cross", 0], ["also", 0], ["known", 0], ["as", 0], ["the", 0], ["way", 0], ["of", 0], ["sorrows", 0], ["or", 0], ["the", 0], ["via", 2], ["crucis", 2], ["refers", 4], ["to", 4], ["a", 6], ["series", 3], ["of", 2], ["images", 2], ["depicting", 2], ["jesus", 2], ["christ", 2], ["on", 2], ["the", 2], ["day", 2], ["of", 2], ["his", 2], ["crucifixion", 2], ["and", 2], ["accompanying", 2], ["prayers", 2], ["the", 3], ["stations", 3], ["grew", 3], ["out", 3], ["of", 3], ["imitations", 3], ["of", 3], ["via", 3], ["dolorosa", 3], ["in", 3], ["jerusalem", 3], ["which", 3], ["is", 2], ["believed", 2], ["to", 2], ["be", 2], ["the", 2], ["actual", 2], ["path", 2], ["jesus", 2], ["walked", 2], ["to", 1], ["mount", 1], ["calvary", 1], ["the", 1], ["object", 1], ["of", 1], ["the", 1], ["stations", 1], ["is", 1], ["to", 1], ["help", 1], ["the", 1], ["christians", 1], ["faithful", 2], ["to", 1], ["make", 1], ["a", 1], ["spiritual", 1], ["pilgrimage", 1], ["through", 1], ["contemplation", 1], ["of", 1], ["the", 1], ["passion", 1], ["of", 1], ["christ", 1], ["it", 1], ["has", 1], ["become", 1], ["one", 1], ["of", 1], ["the", 1], ["most", 1], ["popular", 1], ["devotions", 1], ["and", 1], ["the", 0], ["stations", 0], ["can", 0], ["be", 0], ["found", 0], ["in", 0], ["many", 0], ["western", 0], ["christian", 0], ["churches", 0], ["including", 0], ["anglican", 0], ["1", 0], ["lutheran", 0], ["2", 0], ["methodist", 0], ["3", 0], ["and", 0], ["roman", 0], ["catholic", 0], ["ones", 0], ["question", 0], ["the", 0], ["origins", 0], ["of", 0], ["the", 0], ["stations", 0], ["of", 0], ["the", 0], ["cross", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.4615384615384616, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.7906976744186046, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6317829457364341, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.4615384615384616, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7906976744186046, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6317829457364341, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["via", 3], ["dolorosa", 3], ["in", 3], ["jerusalem", 3], ["which", 3], ["is", 0], ["believed", 0], ["to", 0], ["be", 0], ["the", 0], ["actual", 0], ["path", 0], ["jesus", 0], ["walked", 0], ["to", 0], ["mount", 0], ["calvary", 0], ["pilgrimages", 0], ["to", 0], ["jerusalem", 0], ["and", 0], ["a", 0], ["desire", 0], ["to", 0], ["reproduce", 0], ["via", 0], ["dolorosa", 0], ["grew", 3], ["out", 3], ["of", 3], ["imitations", 3], ["of", 3], ["via", 3], ["dolorosa", 3], ["in", 3], ["jerusalem", 3], ["which", 3], ["is", 0], ["believed", 0], ["to", 0], ["be", 0], ["the", 0], ["actual", 0], ["path", 0], ["jesus", 0], ["walked", 0], ["to", 0], ["mount", 0], ["calvary", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.40540540540540543, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.13513513513513511, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.7959183673469388, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.2653061224489797, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.40540540540540543, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.13513513513513511, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7959183673469388, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.2653061224489797, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Passage: The Methodist revival began with a group of men, including John Wesley (1703–1791) and his younger brother Charles (1707–1788), as a movement within the Church of England in the 18th century.[11][12] The Wesley brothers founded the \"Holy Club\" at the University of Oxford, where John was a fellow and later a lecturer at Lincoln College.[13] The club met weekly and they systematically set about living a holy life. They were accustomed to receiving Communion every week, fasting regularly, abstaining from most forms of amusement and luxury and frequently visited the sick and the poor, as well as prisoners. The fellowship were branded as \"Methodist\" by their fellow students because of the way they used \"rule\" and \"method\" to go about their religious affairs.[1] John, who was leader of the club, took the attempted mockery and turned it into a title of honour.[1][2]\n\nQuestion: Where did the methodist church get its name?", "references": ["because of the way they used \"rule\" and \"method\" to go about their religious affairs"], "id": "id4042"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "openbook_longans"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id4042", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["passage", 0], ["the", 2], ["methodist", 2], ["revival", 2], ["began", 2], ["with", 2], ["a", 2], ["group", 2], ["of", 2], ["men", 2], ["including", 2], ["john", 2], ["wesley", 2], ["1703–1791", 2], ["and", 2], ["his", 2], ["younger", 2], ["brother", 2], ["charles", 2], ["1707–1788", 2], ["as", 1], ["a", 1], ["movement", 0], ["within", 0], ["the", 0], ["church", 0], ["of", 0], ["england", 0], ["in", 0], ["the", 0], ["18th", 0], ["century", 0], ["11", 0], ["12", 0], ["the", 5], ["wesley", 5], ["brothers", 5], ["founded", 5], ["the", 5], ["holy", 5], ["club", 5], ["at", 5], ["the", 5], ["university", 5], ["of", 5], ["oxford", 7], ["where", 0], ["john", 0], ["was", 0], ["a", 0], ["fellow", 0], ["and", 0], ["later", 0], ["a", 0], ["lecturer", 0], ["at", 0], ["lincoln", 0], ["college", 0], ["13", 0], ["the", 0], ["club", 2], ["met", 2], ["weekly", 2], ["and", 2], ["they", 2], ["systematically", 2], ["set", 7], ["about", 7], ["living", 7], ["a", 7], ["holy", 7], ["life", 7], ["they", 7], ["were", 7], ["accustomed", 7], ["to", 7], ["receiving", 7], ["communion", 7], ["every", 7], ["week", 7], ["fasting", 7], ["regularly", 7], ["abstaining", 7], ["from", 2], ["most", 2], ["forms", 2], ["of", 2], ["amusement", 2], ["and", 2], ["luxury", 2], ["and", 2], ["frequently", 2], ["visited", 2], ["the", 2], ["sick", 2], ["and", 2], ["the", 2], ["poor", 2], ["as", 2], ["well", 2], ["as", 2], ["prisoners", 2], ["the", 7], ["fellowship", 7], ["were", 7], ["branded", 7], ["as", 7], ["methodist", 7], ["by", 7], ["their", 7], ["fellow", 7], ["students", 7], ["because", 7], ["of", 7], ["the", 7], ["way", 1], ["they", 0], ["used", 0], ["rule", 0], ["and", 0], ["method", 0], ["to", 0], ["go", 0], ["about", 0], ["their", 0], ["religious", 0], ["affairs", 0], ["1", 0], ["john", 0], ["who", 0], ["was", 0], ["leader", 0], ["of", 0], ["the", 0], ["club", 0], ["took", 0], ["the", 0], ["attempted", 0], ["mockery", 0], ["and", 0], ["turned", 0], ["it", 0], ["into", 0], ["a", 0], ["title", 0], ["of", 0], ["honour", 0], ["1", 0], ["2", 0], ["question", 0], ["where", 0], ["did", 0], ["the", 0], ["methodist", 0], ["church", 0], ["get", 0], ["its", 0], ["name", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6054421768707483, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.21516034985422725, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.7861635220125787, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.5534591194968553, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6054421768707483, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.21516034985422725, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7861635220125787, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.5534591194968553, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["because", 7], ["of", 7], ["the", 7], ["way", 0], ["they", 0], ["used", 0], ["rule", 0], ["and", 0], ["method", 0], ["to", 0], ["go", 0], ["about", 0], ["their", 0], ["religious", 0], ["affairs", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.14285714285714285, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.14285714285714282, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.14285714285714285, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.14285714285714282, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Passage: Beauty and the Beast is a 2017 American musical romantic fantasy film directed by Bill Condon from a screenplay written by Stephen Chbosky and Evan Spiliotopoulos, and co-produced by Walt Disney Pictures and Mandeville Films.[1][6] The film is a live-action adaptation of Disney's 1991 animated film of the same name, itself an adaptation of Jeanne-Marie Leprince de Beaumont's eighteenth-century fairy tale.[7] The film features an ensemble cast that includes Emma Watson and Dan Stevens as the eponymous characters with Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha-Raw, Ian McKellen, and Emma Thompson in supporting roles.[8]\n\nQuestion: Who stars in beauty and the beast 2017?", "references": ["Emma Watson and Dan Stevens as the eponymous characters with Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha-Raw, Ian McKellen, and Emma Thompson in supporting roles"], "id": "id2394"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "openbook_longans"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id2394", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["passage", 0], ["beauty", 3], ["and", 3], ["the", 3], ["beast", 3], ["is", 3], ["a", 3], ["2017", 3], ["american", 3], ["musical", 3], ["romantic", 3], ["fantasy", 4], ["film", 5], ["directed", 5], ["by", 5], ["bill", 4], ["condon", 4], ["from", 4], ["a", 4], ["screenplay", 4], ["written", 2], ["by", 2], ["stephen", 2], ["chbosky", 2], ["and", 2], ["evan", 0], ["spiliotopoulos", 0], ["and", 0], ["co", 0], ["produced", 0], ["by", 0], ["walt", 0], ["disney", 0], ["pictures", 0], ["and", 0], ["mandeville", 0], ["films", 0], ["1", 0], ["6", 0], ["the", 0], ["film", 0], ["is", 0], ["a", 0], ["live", 0], ["action", 0], ["adaptation", 0], ["of", 8], ["disney", 8], ["s", 8], ["1991", 9], ["animated", 9], ["film", 9], ["of", 8], ["the", 8], ["same", 0], ["name", 0], ["itself", 0], ["an", 1], ["adaptation", 0], ["of", 0], ["jeanne", 0], ["marie", 0], ["leprince", 0], ["de", 0], ["beaumont", 0], ["s", 0], ["eighteenth", 0], ["century", 0], ["fairy", 0], ["tale", 0], ["7", 0], ["the", 3], ["film", 3], ["features", 3], ["an", 0], ["ensemble", 0], ["cast", 0], ["that", 0], ["includes", 0], ["emma", 0], ["watson", 0], ["and", 0], ["dan", 0], ["stevens", 0], ["as", 0], ["the", 0], ["eponymous", 0], ["characters", 3], ["with", 4], ["luke", 6], ["evans", 6], ["kevin", 6], ["kline", 6], ["josh", 6], ["gad", 6], ["ewan", 8], ["mcgregor", 8], ["stanley", 3], ["tucci", 2], ["audra", 2], ["mcdonald", 0], ["gugu", 0], ["mbatha", 0], ["raw", 0], ["ian", 0], ["mckellen", 0], ["and", 0], ["emma", 0], ["thompson", 0], ["in", 0], ["supporting", 0], ["roles", 0], ["8", 0], ["question", 0], ["who", 0], ["stars", 0], ["in", 0], ["beauty", 0], ["and", 0], ["the", 0], ["beast", 0], ["2017", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.44144144144144143, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.12665165165165163, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.8130081300813008, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.364837398373984, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.44144144144144143, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.12665165165165163, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.8130081300813008, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.364837398373984, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["emma", 0], ["watson", 0], ["and", 0], ["dan", 0], ["stevens", 0], ["as", 0], ["the", 0], ["eponymous", 0], ["characters", 3], ["with", 4], ["luke", 6], ["evans", 6], ["kevin", 6], ["kline", 6], ["josh", 6], ["gad", 6], ["ewan", 8], ["mcgregor", 8], ["stanley", 3], ["tucci", 2], ["audra", 2], ["mcdonald", 0], ["gugu", 0], ["mbatha", 0], ["raw", 0], ["ian", 0], ["mckellen", 0], ["and", 0], ["emma", 0], ["thompson", 0], ["in", 0], ["supporting", 0], ["roles", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6190476190476191, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.1507936507936508, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.7575757575757576, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.32323232323232326, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6190476190476191, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.1507936507936508, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7575757575757576, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.32323232323232326, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
