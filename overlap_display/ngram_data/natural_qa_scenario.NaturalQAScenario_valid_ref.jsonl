{"instance": {"input": "Who stars in beauty and the beast 2017?", "references": ["Emma Watson and Dan Stevens as the eponymous characters with Luke Evans, Kevin Kline, Josh Gad, Ewan McGregor, Stanley Tucci, Audra McDonald, Gugu Mbatha-Raw, Ian McKellen, and Emma Thompson in supporting roles"], "id": "id2394"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id2394", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["who", 0], ["stars", 0], ["in", 0], ["beauty", 0], ["and", 0], ["the", 0], ["beast", 0], ["2017", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["emma", 0], ["watson", 0], ["and", 0], ["dan", 0], ["stevens", 0], ["as", 0], ["the", 0], ["eponymous", 0], ["characters", 3], ["with", 4], ["luke", 6], ["evans", 6], ["kevin", 6], ["kline", 6], ["josh", 6], ["gad", 6], ["ewan", 8], ["mcgregor", 8], ["stanley", 3], ["tucci", 2], ["audra", 2], ["mcdonald", 0], ["gugu", 0], ["mbatha", 0], ["raw", 0], ["ian", 0], ["mckellen", 0], ["and", 0], ["emma", 0], ["thompson", 0], ["in", 0], ["supporting", 0], ["roles", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6190476190476191, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.1507936507936508, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.7575757575757576, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.32323232323232326, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6190476190476191, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.1507936507936508, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7575757575757576, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.32323232323232326, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What is a cup measurement in american recipes?", "references": ["equal to half a liquid pint", "traditionally equal to half a liquid pint in either US customary units or the British imperial system but is now separately defined in terms of the metric system at values between  1⁄5 and  1⁄4 of a liter"], "id": "id3446"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3446", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["is", 0], ["a", 0], ["cup", 0], ["measurement", 0], ["in", 0], ["american", 0], ["recipes", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["equal", 0], ["to", 0], ["half", 0], ["a", 0], ["liquid", 0], ["pint", 0], ["traditionally", 6], ["equal", 6], ["to", 6], ["half", 6], ["a", 6], ["liquid", 6], ["pint", 6], ["in", 6], ["either", 6], ["us", 6], ["customary", 6], ["units", 6], ["or", 6], ["the", 6], ["british", 6], ["imperial", 6], ["system", 6], ["but", 7], ["is", 7], ["now", 6], ["separately", 6], ["defined", 6], ["in", 6], ["terms", 6], ["of", 6], ["the", 0], ["metric", 0], ["system", 0], ["at", 0], ["values", 0], ["between", 0], ["1⁄5", 0], ["and", 0], ["1⁄4", 0], ["of", 0], ["a", 0], ["liter", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.8064516129032258, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.13287250384024574, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.8604651162790697, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.14341085271317836, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.8064516129032258, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.13287250384024574, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.8604651162790697, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.14341085271317836, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Which note is middle c on a piano?", "references": ["the fourth C key from left", "the fourth C key from left on a standard 88-key piano keyboard"], "id": "id2474"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id2474", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["which", 0], ["note", 0], ["is", 0], ["middle", 0], ["c", 0], ["on", 0], ["a", 0], ["piano", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["the", 0], ["fourth", 0], ["c", 0], ["key", 0], ["from", 0], ["left", 0], ["the", 1], ["fourth", 0], ["c", 0], ["key", 0], ["from", 0], ["left", 0], ["on", 0], ["a", 0], ["standard", 0], ["88", 0], ["key", 0], ["piano", 0], ["keyboard", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.14285714285714285, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.14285714285714285, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.6842105263157895, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6842105263157895, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.14285714285714285, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.14285714285714285, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.6842105263157895, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6842105263157895, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What does fancy dress mean in the uk?", "references": ["A costume", "a type of party, common mainly in contemporary Western culture, where guests dress up in costumes", "guests dress up in costumes", "costumes"], "id": "id1586"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id1586", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["does", 0], ["fancy", 0], ["dress", 0], ["mean", 0], ["in", 0], ["the", 0], ["uk", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["a", 0], ["costume", 0], ["a", 1], ["type", 1], ["of", 1], ["party", 1], ["common", 0], ["mainly", 0], ["in", 0], ["contemporary", 0], ["western", 0], ["culture", 0], ["where", 0], ["guests", 0], ["dress", 0], ["up", 0], ["in", 0], ["costumes", 0], ["guests", 0], ["dress", 0], ["up", 0], ["in", 0], ["costumes", 0], ["costumes", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.3333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.3333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.6666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.3333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.3333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.6666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Where do green tree frogs go during the day?", "references": ["cool, dark, and moist areas, such as tree holes or rock crevices", "cool, dark, and moist areas, such as tree holes or rock crevices, in which to sleep"], "id": "id2952"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id2952", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["where", 0], ["do", 0], ["green", 0], ["tree", 0], ["frogs", 0], ["go", 0], ["during", 0], ["the", 0], ["day", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["cool", 0], ["dark", 0], ["and", 0], ["moist", 0], ["areas", 0], ["such", 0], ["as", 0], ["tree", 0], ["holes", 0], ["or", 0], ["rock", 0], ["crevices", 0], ["cool", 1], ["dark", 1], ["and", 1], ["moist", 1], ["areas", 0], ["such", 0], ["as", 0], ["tree", 0], ["holes", 0], ["or", 0], ["rock", 0], ["crevices", 0], ["in", 0], ["which", 0], ["to", 0], ["sleep", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.25, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.25, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.5714285714285714, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.5714285714285714, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.25, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.25, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.5714285714285714, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.5714285714285714, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What is the definition of glory in the bible?", "references": ["the manifestation of God's presence as perceived by humans", "St. Augustine renders it as clara notitia cum laude, \"brilliant celebrity with praise\""], "id": "id1966"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id1966", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["is", 0], ["the", 0], ["definition", 0], ["of", 0], ["glory", 0], ["in", 0], ["the", 0], ["bible", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["the", 0], ["manifestation", 0], ["of", 0], ["god", 0], ["s", 0], ["presence", 0], ["as", 0], ["perceived", 0], ["by", 0], ["humans", 0], ["st", 1], ["augustine", 0], ["renders", 0], ["it", 0], ["as", 0], ["clara", 0], ["notitia", 0], ["cum", 0], ["laude", 0], ["brilliant", 0], ["celebrity", 0], ["with", 0], ["praise", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.08333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.08333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.5416666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.5416666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.08333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.08333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.5416666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.5416666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Explain the function of the peripheral nervous system?", "references": ["to connect the CNS to the limbs and organs, essentially serving as a relay between the brain and spinal cord and the rest of the body"], "id": "id1166"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id1166", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["explain", 0], ["the", 0], ["function", 0], ["of", 0], ["the", 0], ["peripheral", 0], ["nervous", 0], ["system", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["to", 2], ["connect", 1], ["the", 1], ["cns", 1], ["to", 1], ["the", 1], ["limbs", 1], ["and", 1], ["organs", 1], ["essentially", 1], ["serving", 1], ["as", 1], ["a", 1], ["relay", 2], ["between", 0], ["the", 0], ["brain", 0], ["and", 0], ["spinal", 0], ["cord", 0], ["and", 0], ["the", 0], ["rest", 0], ["of", 0], ["the", 0], ["body", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.9285714285714286, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.9807692307692307, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.9285714285714286, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.9807692307692307, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Who has won the eurovision song contest the most times?", "references": ["Ireland", "Johnny Logan, who performed \"What's Another Year\" in 1980 and \"Hold Me Now\" in 1987", "Ireland's Johnny Logan"], "id": "id3312"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3312", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["who", 0], ["has", 0], ["won", 0], ["the", 0], ["eurovision", 0], ["song", 0], ["contest", 0], ["the", 0], ["most", 0], ["times", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["ireland", 0], ["johnny", 1], ["logan", 1], ["who", 1], ["performed", 1], ["what", 0], ["s", 0], ["another", 0], ["year", 0], ["in", 0], ["1980", 0], ["and", 0], ["hold", 0], ["me", 0], ["now", 0], ["in", 0], ["1987", 0], ["ireland", 0], ["s", 0], ["johnny", 0], ["logan", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.4444444444444444, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.4444444444444444, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.7619047619047619, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.7619047619047619, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.4444444444444444, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.4444444444444444, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7619047619047619, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.7619047619047619, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What does disk cleanup mean on a computer?", "references": ["a computer maintenance utility included in Microsoft Windows designed to free up disk space on a computer's hard drive"], "id": "id2274"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id2274", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["does", 0], ["disk", 0], ["cleanup", 0], ["mean", 0], ["on", 0], ["a", 0], ["computer", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["a", 1], ["computer", 1], ["maintenance", 1], ["utility", 1], ["included", 1], ["in", 1], ["microsoft", 1], ["windows", 1], ["designed", 0], ["to", 0], ["free", 0], ["up", 0], ["disk", 0], ["space", 0], ["on", 0], ["a", 0], ["computer", 0], ["s", 0], ["hard", 0], ["drive", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "The origins of the stations of the cross?", "references": ["Via Dolorosa in Jerusalem which is believed to be the actual path Jesus walked to Mount Calvary", "pilgrimages to Jerusalem and a desire to reproduce Via Dolorosa", "grew out of imitations of Via Dolorosa in Jerusalem which is believed to be the actual path Jesus walked to Mount Calvary"], "id": "id936"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id936", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["the", 0], ["origins", 0], ["of", 0], ["the", 0], ["stations", 0], ["of", 0], ["the", 0], ["cross", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["via", 3], ["dolorosa", 3], ["in", 3], ["jerusalem", 3], ["which", 3], ["is", 0], ["believed", 0], ["to", 0], ["be", 0], ["the", 0], ["actual", 0], ["path", 0], ["jesus", 0], ["walked", 0], ["to", 0], ["mount", 0], ["calvary", 0], ["pilgrimages", 0], ["to", 0], ["jerusalem", 0], ["and", 0], ["a", 0], ["desire", 0], ["to", 0], ["reproduce", 0], ["via", 0], ["dolorosa", 0], ["grew", 3], ["out", 3], ["of", 3], ["imitations", 3], ["of", 3], ["via", 3], ["dolorosa", 3], ["in", 3], ["jerusalem", 3], ["which", 3], ["is", 0], ["believed", 0], ["to", 0], ["be", 0], ["the", 0], ["actual", 0], ["path", 0], ["jesus", 0], ["walked", 0], ["to", 0], ["mount", 0], ["calvary", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.40540540540540543, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.13513513513513511, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.7959183673469388, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.2653061224489797, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.40540540540540543, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.13513513513513511, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7959183673469388, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.2653061224489797, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What is the source of information for account receivables?", "references": ["invoices", "invoices raised by a business and delivered to the customer for payment within an agreed time frame"], "id": "id635"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id635", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["is", 0], ["the", 0], ["source", 0], ["of", 0], ["information", 0], ["for", 0], ["account", 0], ["receivables", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["invoices", 0], ["invoices", 2], ["raised", 2], ["by", 2], ["a", 2], ["business", 2], ["and", 0], ["delivered", 0], ["to", 0], ["the", 0], ["customer", 0], ["for", 0], ["payment", 0], ["within", 0], ["an", 0], ["agreed", 0], ["time", 0], ["frame", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.8333333333333334, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.4166666666666667, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.9444444444444444, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.4722222222222222, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.8333333333333334, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.4166666666666667, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.9444444444444444, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.4722222222222222, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Where is salamis in ancient greece on a map?", "references": ["in the Saronic Gulf, about 1 nautical mile (2 km) off-coast from Piraeus and about 16 kilometres (10 miles) west of Athens", "about 1 nautical mile (2 km) off-coast from Piraeus and about 16 kilometres (10 miles) west of Athens", "1 nautical mile (2 km) off-coast from Piraeus and about 16 kilometres (10 miles) west of Athens"], "id": "id4132"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id4132", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["where", 0], ["is", 0], ["salamis", 0], ["in", 0], ["ancient", 0], ["greece", 0], ["on", 0], ["a", 0], ["map", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["in", 1], ["the", 1], ["saronic", 1], ["gulf", 1], ["about", 1], ["1", 0], ["nautical", 0], ["mile", 0], ["2", 0], ["km", 0], ["off", 0], ["coast", 0], ["from", 0], ["piraeus", 0], ["and", 0], ["about", 0], ["16", 0], ["kilometres", 0], ["10", 0], ["miles", 0], ["west", 0], ["of", 0], ["athens", 0], ["about", 1], ["1", 0], ["nautical", 0], ["mile", 0], ["2", 0], ["km", 0], ["off", 0], ["coast", 0], ["from", 0], ["piraeus", 0], ["and", 0], ["about", 0], ["16", 0], ["kilometres", 0], ["10", 0], ["miles", 0], ["west", 0], ["of", 0], ["athens", 0], ["1", 0], ["nautical", 0], ["mile", 0], ["2", 0], ["km", 0], ["off", 0], ["coast", 0], ["from", 0], ["piraeus", 0], ["and", 0], ["about", 0], ["16", 0], ["kilometres", 0], ["10", 0], ["miles", 0], ["west", 0], ["of", 0], ["athens", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.125, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.125, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.125, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.125, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What is the primary purpose for adding flash elements to a website?", "references": ["to display interactive web pages, online games, and to playback video and audio content"], "id": "id1792"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id1792", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["is", 0], ["the", 0], ["primary", 0], ["purpose", 0], ["for", 0], ["adding", 0], ["flash", 0], ["elements", 0], ["to", 0], ["a", 0], ["website", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["to", 2], ["display", 2], ["interactive", 0], ["web", 0], ["pages", 0], ["online", 0], ["games", 0], ["and", 0], ["to", 0], ["playback", 0], ["video", 0], ["and", 0], ["audio", 0], ["content", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Who do you play as in dragon age origins?", "references": ["warrior, mage, or rogue coming from an elven, human, or dwarven background", "a warrior, mage, or rogue coming from an elven, human, or dwarven background"], "id": "id2408"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id2408", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["who", 0], ["do", 0], ["you", 0], ["play", 0], ["as", 0], ["in", 0], ["dragon", 0], ["age", 0], ["origins", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["warrior", 0], ["mage", 0], ["or", 0], ["rogue", 0], ["coming", 0], ["from", 0], ["an", 0], ["elven", 0], ["human", 0], ["or", 0], ["dwarven", 0], ["background", 0], ["a", 1], ["warrior", 0], ["mage", 0], ["or", 0], ["rogue", 0], ["coming", 0], ["from", 0], ["an", 0], ["elven", 0], ["human", 0], ["or", 0], ["dwarven", 0], ["background", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.07692307692307693, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.07692307692307693, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.52, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.52, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.07692307692307693, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.07692307692307693, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.52, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.52, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What is the role of the us attorney general?", "references": ["the head of the United States Department of Justice per 28 U.S.C. § 503, concerned with all legal affairs, and is the chief lawyer of the United States government", "to prosecute and conduct all suits in the Supreme Court in which the United States shall be concerned, and to give his or her advice and opinion upon questions of law when required by the President of the United States, or when requested by the heads of any of the departments"], "id": "id4040"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id4040", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["is", 0], ["the", 0], ["role", 0], ["of", 0], ["the", 0], ["us", 0], ["attorney", 0], ["general", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["the", 4], ["head", 4], ["of", 0], ["the", 0], ["united", 0], ["states", 0], ["department", 0], ["of", 0], ["justice", 0], ["per", 0], ["28", 0], ["u", 0], ["s", 0], ["c", 0], ["§", 0], ["503", 0], ["concerned", 0], ["with", 0], ["all", 0], ["legal", 0], ["affairs", 0], ["and", 0], ["is", 0], ["the", 0], ["chief", 0], ["lawyer", 0], ["of", 0], ["the", 0], ["united", 0], ["states", 0], ["government", 0], ["to", 5], ["prosecute", 8], ["and", 8], ["conduct", 8], ["all", 8], ["suits", 8], ["in", 8], ["the", 6], ["supreme", 6], ["court", 6], ["in", 0], ["which", 0], ["the", 0], ["united", 0], ["states", 0], ["shall", 0], ["be", 0], ["concerned", 0], ["and", 0], ["to", 0], ["give", 0], ["his", 0], ["or", 0], ["her", 0], ["advice", 8], ["and", 8], ["opinion", 8], ["upon", 8], ["questions", 8], ["of", 8], ["law", 8], ["when", 8], ["required", 8], ["by", 8], ["the", 8], ["president", 7], ["of", 7], ["the", 7], ["united", 6], ["states", 0], ["or", 0], ["when", 0], ["requested", 0], ["by", 0], ["the", 0], ["heads", 0], ["of", 0], ["any", 0], ["of", 0], ["the", 0], ["departments", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.38571428571428573, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.056003401360544214, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.7682926829268293, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.1447590011614401, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.38571428571428573, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.056003401360544214, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7682926829268293, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.1447590011614401, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What written material is included in the talmud?", "references": ["the Mishnah (Hebrew: משנה, c. 200 CE), a written compendium of Rabbinic Judaism's Oral Torah", "the Gemara (c. 500 CE), an elucidation of the Mishnah and related Tannaitic writings that often ventures onto other subjects and expounds broadly on the Hebrew Bible", "the Mishnah", "the Gemara"], "id": "id787"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id787", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["written", 0], ["material", 0], ["is", 0], ["included", 0], ["in", 0], ["the", 0], ["talmud", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["the", 0], ["mishnah", 0], ["hebrew", 0], ["משנה", 0], ["c", 0], ["200", 0], ["ce", 0], ["a", 0], ["written", 0], ["compendium", 0], ["of", 0], ["rabbinic", 0], ["judaism", 0], ["s", 0], ["oral", 0], ["torah", 0], ["the", 1], ["gemara", 1], ["c", 1], ["500", 1], ["ce", 1], ["an", 3], ["elucidation", 3], ["of", 6], ["the", 6], ["mishnah", 6], ["and", 6], ["related", 6], ["tannaitic", 6], ["writings", 2], ["that", 2], ["often", 0], ["ventures", 0], ["onto", 0], ["other", 0], ["subjects", 0], ["and", 0], ["expounds", 0], ["broadly", 0], ["on", 0], ["the", 0], ["hebrew", 0], ["bible", 0], ["the", 0], ["mishnah", 0], ["the", 0], ["gemara", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.42857142857142855, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.21904761904761907, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.574468085106383, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.574468085106383, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.42857142857142855, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.21904761904761907, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.574468085106383, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.574468085106383, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "How does the mystery spot in santa cruz work?", "references": ["results from the oddly tilted environment as well as standing on a tilted floor", "gravity hill, tilt-induced visual illusion"], "id": "id3670"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3670", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["how", 0], ["does", 0], ["the", 0], ["mystery", 0], ["spot", 0], ["in", 0], ["santa", 0], ["cruz", 0], ["work", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["results", 2], ["from", 2], ["the", 0], ["oddly", 0], ["tilted", 0], ["environment", 0], ["as", 0], ["well", 0], ["as", 0], ["standing", 0], ["on", 0], ["a", 0], ["tilted", 0], ["floor", 0], ["gravity", 0], ["hill", 0], ["tilt", 0], ["induced", 0], ["visual", 0], ["illusion", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.25, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.125, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.7, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.35, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.25, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.125, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.35, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What is the purpose of a website kick starter?", "references": ["global crowdfunding platform focused on creativity and merchandising", "crowdfunding platforms for gathering money from the public, which circumvents traditional avenues of investment"], "id": "id2332"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id2332", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["is", 0], ["the", 0], ["purpose", 0], ["of", 0], ["a", 0], ["website", 0], ["kick", 0], ["starter", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["global", 0], ["crowdfunding", 0], ["platform", 0], ["focused", 0], ["on", 0], ["creativity", 0], ["and", 0], ["merchandising", 0], ["crowdfunding", 3], ["platforms", 3], ["for", 0], ["gathering", 0], ["money", 0], ["from", 0], ["the", 0], ["public", 0], ["which", 0], ["circumvents", 0], ["traditional", 0], ["avenues", 0], ["of", 0], ["investment", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.2, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.06666666666666667, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.6363636363636364, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.2121212121212121, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.2, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.06666666666666667, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.6363636363636364, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.2121212121212121, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "In ready player one what does oasis stand for?", "references": ["a virtual reality simulator accessible by players using visors and haptic technology such as gloves"], "id": "id757"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id757", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["in", 0], ["ready", 0], ["player", 0], ["one", 0], ["what", 0], ["does", 0], ["oasis", 0], ["stand", 0], ["for", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["a", 1], ["virtual", 1], ["reality", 1], ["simulator", 0], ["accessible", 0], ["by", 0], ["players", 0], ["using", 0], ["visors", 0], ["and", 0], ["haptic", 0], ["technology", 0], ["such", 0], ["as", 0], ["gloves", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What is the secret of the law of attraction?", "references": ["the belief that by focusing on positive or negative thoughts people can bring positive or negative experiences into their life"], "id": "id1360"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id1360", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["is", 0], ["the", 0], ["secret", 0], ["of", 0], ["the", 0], ["law", 0], ["of", 0], ["attraction", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["the", 40], ["belief", 41], ["that", 41], ["by", 41], ["focusing", 42], ["on", 42], ["positive", 42], ["or", 42], ["negative", 0], ["thoughts", 0], ["people", 0], ["can", 0], ["bring", 0], ["positive", 0], ["or", 0], ["negative", 0], ["experiences", 0], ["into", 0], ["their", 0], ["life", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.02417610336817654, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.025000000000000005, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What is the wavelength of the electromagnetic wave?", "references": ["from 7023239999999999999♠2.4×1023 Hz (1 GeV gamma rays) down to the local plasma frequency of the ionized interstellar medium (~1 kHz)", "ranging from below one hertz to above 1025 hertz"], "id": "id3978"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3978", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["is", 0], ["the", 0], ["wavelength", 0], ["of", 0], ["the", 0], ["electromagnetic", 0], ["wave", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["from", 0], ["7023239999999999999♠2", 0], ["4×1023", 1], ["hz", 2], ["1", 2], ["gev", 2], ["gamma", 2], ["rays", 2], ["down", 2], ["to", 0], ["the", 0], ["local", 0], ["plasma", 0], ["frequency", 0], ["of", 0], ["the", 0], ["ionized", 0], ["interstellar", 0], ["medium", 0], ["1", 0], ["khz", 0], ["ranging", 0], ["from", 0], ["below", 0], ["one", 0], ["hertz", 0], ["to", 0], ["above", 0], ["1025", 0], ["hertz", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.3888888888888889, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.2222222222222222, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.6333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.3888888888888889, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.2222222222222222, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.6333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What does the application of contract of adhesion mean?", "references": ["a contract between two parties, where the terms and conditions of the contract are set by one of the parties, and the other party has little or no ability to negotiate more favorable terms and is thus placed in a \"take it or leave it\" position"], "id": "id3900"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3900", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["does", 0], ["the", 0], ["application", 0], ["of", 0], ["contract", 0], ["of", 0], ["adhesion", 0], ["mean", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["a", 0], ["contract", 1], ["between", 1], ["two", 1], ["parties", 1], ["where", 2], ["the", 2], ["terms", 2], ["and", 2], ["conditions", 2], ["of", 2], ["the", 2], ["contract", 2], ["are", 2], ["set", 2], ["by", 2], ["one", 2], ["of", 2], ["the", 2], ["parties", 2], ["and", 2], ["the", 2], ["other", 2], ["party", 2], ["has", 2], ["little", 2], ["or", 2], ["no", 2], ["ability", 2], ["to", 2], ["negotiate", 2], ["more", 2], ["favorable", 2], ["terms", 2], ["and", 0], ["is", 0], ["thus", 0], ["placed", 0], ["in", 0], ["a", 0], ["take", 0], ["it", 0], ["or", 0], ["leave", 0], ["it", 0], ["position", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.9705882352941176, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.5441176470588235, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.9782608695652174, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.9782608695652174, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.9705882352941176, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.5441176470588235, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.9782608695652174, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.9782608695652174, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What was the outcome of the famous 1954 case of brown v. board of education of topeka?", "references": ["the Court declared state laws establishing separate public schools for black and white students to be unconstitutional", "declared state laws establishing separate public schools for black and white students to be unconstitutional"], "id": "id2258"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id2258", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["was", 0], ["the", 0], ["outcome", 0], ["of", 0], ["the", 0], ["famous", 0], ["1954", 0], ["case", 0], ["of", 0], ["brown", 0], ["v", 0], ["board", 0], ["of", 0], ["education", 0], ["of", 0], ["topeka", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["the", 15], ["court", 17], ["declared", 17], ["state", 18], ["laws", 18], ["establishing", 0], ["separate", 0], ["public", 0], ["schools", 0], ["for", 0], ["black", 0], ["and", 0], ["white", 0], ["students", 0], ["to", 0], ["be", 0], ["unconstitutional", 0], ["declared", 17], ["state", 18], ["laws", 18], ["establishing", 0], ["separate", 0], ["public", 0], ["schools", 0], ["for", 0], ["black", 0], ["and", 0], ["white", 0], ["students", 0], ["to", 0], ["be", 0], ["unconstitutional", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.4, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.02326797385620915, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.06666666666666667, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What's with the ashes on ash wednesday?", "references": ["reminds worshippers of their sinfulness and mortality and thus, implicitly, of their need to repent in time"], "id": "id3504"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3504", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["s", 0], ["with", 0], ["the", 0], ["ashes", 0], ["on", 0], ["ash", 0], ["wednesday", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["reminds", 2], ["worshippers", 2], ["of", 2], ["their", 2], ["sinfulness", 2], ["and", 0], ["mortality", 0], ["and", 0], ["thus", 0], ["implicitly", 0], ["of", 0], ["their", 0], ["need", 0], ["to", 0], ["repent", 0], ["in", 0], ["time", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Where can carbon be found in the biosphere?", "references": ["in all land-living organisms, both alive and dead, as well as carbon stored in soils", "plants", "other living organisms", "soil", "The terrestrial biosphere"], "id": "id1642"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id1642", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["where", 0], ["can", 0], ["carbon", 0], ["be", 0], ["found", 0], ["in", 0], ["the", 0], ["biosphere", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["in", 1], ["all", 1], ["land", 1], ["living", 1], ["organisms", 0], ["both", 0], ["alive", 0], ["and", 0], ["dead", 0], ["as", 0], ["well", 0], ["as", 0], ["carbon", 0], ["stored", 0], ["in", 0], ["soils", 0], ["plants", 0], ["other", 0], ["living", 0], ["organisms", 0], ["soil", 0], ["the", 0], ["terrestrial", 0], ["biosphere", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.3333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.3333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.6666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.3333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.3333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.6666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Where does the brain get its energy from?", "references": ["most of its energy from oxygen-dependent metabolism of glucose (i.e., blood sugar),[73] but ketones provide a major alternative source, together with contributions from medium chain fatty acids (caprylic[78] and heptanoic[79] acids), lactate,[80] acetate,[81] and possibly amino acids", "most of its energy from oxygen-dependent metabolism of glucose (i.e., blood sugar)"], "id": "id79"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id79", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["where", 0], ["does", 0], ["the", 0], ["brain", 0], ["get", 0], ["its", 0], ["energy", 0], ["from", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["most", 1], ["of", 1], ["its", 0], ["energy", 0], ["from", 0], ["oxygen", 0], ["dependent", 0], ["metabolism", 0], ["of", 0], ["glucose", 0], ["i", 0], ["e", 0], ["blood", 0], ["sugar", 0], ["73", 0], ["but", 0], ["ketones", 0], ["provide", 0], ["a", 0], ["major", 0], ["alternative", 0], ["source", 0], ["together", 0], ["with", 0], ["contributions", 0], ["from", 0], ["medium", 0], ["chain", 0], ["fatty", 0], ["acids", 0], ["caprylic", 0], ["78", 0], ["and", 0], ["heptanoic", 0], ["79", 0], ["acids", 0], ["lactate", 0], ["80", 0], ["acetate", 0], ["81", 0], ["and", 0], ["possibly", 0], ["amino", 0], ["acids", 0], ["most", 1], ["of", 1], ["its", 0], ["energy", 0], ["from", 0], ["oxygen", 0], ["dependent", 0], ["metabolism", 0], ["of", 0], ["glucose", 0], ["i", 0], ["e", 0], ["blood", 0], ["sugar", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.0851063829787234, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.0851063829787234, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.4745762711864407, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.4745762711864407, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0851063829787234, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0851063829787234, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.4745762711864407, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.4745762711864407, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "How many gallons of water in the gulf of mexico?", "references": ["2,500 quadrillion liters (550 quadrillion Imperial gallons, 660 quadrillion US gallons, 2.5 million km3 or 600,000 cu mi)", "660 quadrillion US gallons"], "id": "id3734"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3734", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["how", 0], ["many", 0], ["gallons", 0], ["of", 0], ["water", 0], ["in", 0], ["the", 0], ["gulf", 0], ["of", 0], ["mexico", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["2", 1], ["500", 1], ["quadrillion", 1], ["liters", 1], ["550", 1], ["quadrillion", 1], ["imperial", 1], ["gallons", 1], ["660", 1], ["quadrillion", 0], ["us", 0], ["gallons", 0], ["2", 0], ["5", 0], ["million", 0], ["km3", 0], ["or", 0], ["600", 0], ["000", 0], ["cu", 0], ["mi", 0], ["660", 0], ["quadrillion", 0], ["us", 0], ["gallons", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6923076923076923, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6923076923076923, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.84, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.84, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6923076923076923, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6923076923076923, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.84, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.84, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "I can't go for that lyrics meaning?", "references": ["about not being pushed around by big labels, managers, and agents and being told what to do, and being true to yourself creatively"], "id": "id4046"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id4046", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["i", 0], ["can", 0], ["t", 0], ["go", 0], ["for", 0], ["that", 0], ["lyrics", 0], ["meaning", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["about", 1], ["not", 1], ["being", 1], ["pushed", 1], ["around", 1], ["by", 1], ["big", 1], ["labels", 1], ["managers", 1], ["and", 1], ["agents", 1], ["and", 0], ["being", 0], ["told", 0], ["what", 0], ["to", 0], ["do", 0], ["and", 0], ["being", 0], ["true", 0], ["to", 0], ["yourself", 0], ["creatively", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What is the significance of the sahara desert?", "references": ["the largest hot desert and the third largest desert in the world after Antarctica and the Arctic", "the largest hot desert", "the third largest desert in the world after Antarctica and the Arctic"], "id": "id197"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id197", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["is", 0], ["the", 0], ["significance", 0], ["of", 0], ["the", 0], ["sahara", 0], ["desert", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["the", 1], ["largest", 1], ["hot", 1], ["desert", 1], ["and", 1], ["the", 0], ["third", 0], ["largest", 0], ["desert", 0], ["in", 0], ["the", 0], ["world", 0], ["after", 0], ["antarctica", 0], ["and", 0], ["the", 0], ["arctic", 0], ["the", 0], ["largest", 0], ["hot", 0], ["desert", 0], ["the", 0], ["third", 0], ["largest", 0], ["desert", 0], ["in", 0], ["the", 0], ["world", 0], ["after", 0], ["antarctica", 0], ["and", 0], ["the", 0], ["arctic", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.23809523809523808, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.23809523809523808, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.5151515151515151, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.5151515151515151, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.23809523809523808, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.23809523809523808, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.5151515151515151, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.5151515151515151, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What does it mean on tinder common connections?", "references": ["Common Connections allows users to see whether they share a mutual Facebook friend with a match (a first degree connection on Tinder) or when a user and their match have two separate friends who happen to be friends with each other (considered second degree on Tinder)"], "id": "id3272"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3272", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["does", 0], ["it", 0], ["mean", 0], ["on", 0], ["tinder", 0], ["common", 0], ["connections", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["common", 0], ["connections", 0], ["allows", 0], ["users", 0], ["to", 0], ["see", 0], ["whether", 0], ["they", 0], ["share", 0], ["a", 0], ["mutual", 0], ["facebook", 0], ["friend", 0], ["with", 0], ["a", 0], ["match", 0], ["a", 0], ["first", 0], ["degree", 0], ["connection", 0], ["on", 0], ["tinder", 0], ["or", 0], ["when", 0], ["a", 0], ["user", 0], ["and", 0], ["their", 0], ["match", 1], ["have", 0], ["two", 0], ["separate", 0], ["friends", 0], ["who", 0], ["happen", 0], ["to", 0], ["be", 0], ["friends", 0], ["with", 0], ["each", 0], ["other", 0], ["considered", 0], ["second", 0], ["degree", 0], ["on", 0], ["tinder", 0], ["", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.02857142857142857, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.02857142857142857, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.2765957446808511, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.2765957446808511, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.02857142857142857, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.02857142857142857, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.2765957446808511, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.2765957446808511, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Where was the tv show in the heat of the night filmed?", "references": ["Covington, Georgia", "Georgia counties of Newton (where Covington is located), Rockdale, Walton, Morgan, and Jasper", "Decatur in Dekalb County", "Atlanta", "Hammond, Louisiana", "the Georgia counties of Newton (where Covington is located), Rockdale, Walton, Morgan, and Jasper"], "id": "id2382"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id2382", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["where", 0], ["was", 0], ["the", 0], ["tv", 0], ["show", 0], ["in", 0], ["the", 0], ["heat", 0], ["of", 0], ["the", 0], ["night", 0], ["filmed", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["covington", 0], ["georgia", 0], ["georgia", 1], ["counties", 0], ["of", 0], ["newton", 0], ["where", 0], ["covington", 0], ["is", 0], ["located", 0], ["rockdale", 0], ["walton", 0], ["morgan", 0], ["and", 0], ["jasper", 0], ["decatur", 0], ["in", 0], ["dekalb", 0], ["county", 0], ["atlanta", 0], ["hammond", 0], ["louisiana", 0], ["the", 1], ["georgia", 1], ["counties", 0], ["of", 0], ["newton", 0], ["where", 0], ["covington", 0], ["is", 0], ["located", 0], ["rockdale", 0], ["walton", 0], ["morgan", 0], ["and", 0], ["jasper", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.125, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.125, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.75, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.75, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.125, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.125, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.75, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.75, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "How many us troops does the united states have?", "references": ["1,281,900 servicemembers,[4] with an additional 801,200 people in the seven reserve components"], "id": "id3660"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3660", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["how", 0], ["many", 0], ["us", 0], ["troops", 0], ["does", 0], ["the", 0], ["united", 0], ["states", 0], ["have", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["1", 16], ["281", 16], ["900", 16], ["servicemembers", 16], ["4", 0], ["with", 0], ["an", 0], ["additional", 0], ["801", 0], ["200", 0], ["people", 0], ["in", 0], ["the", 0], ["seven", 0], ["reserve", 0], ["components", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.0625, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.0625, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Who did shakespeare write his play hamlet for?", "references": ["almost certainly wrote his version of the title role for his fellow actor, Richard Burbage", "his fellow actor, Richard Burbage", "for his fellow actor, Richard Burbage, the leading tragedian of Shakespeare's time"], "id": "id47"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id47", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["who", 0], ["did", 0], ["shakespeare", 0], ["write", 0], ["his", 0], ["play", 0], ["hamlet", 0], ["for", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["almost", 2], ["certainly", 2], ["wrote", 2], ["his", 0], ["version", 0], ["of", 0], ["the", 0], ["title", 0], ["role", 0], ["for", 0], ["his", 0], ["fellow", 0], ["actor", 0], ["richard", 0], ["burbage", 0], ["his", 0], ["fellow", 0], ["actor", 0], ["richard", 0], ["burbage", 0], ["for", 0], ["his", 0], ["fellow", 0], ["actor", 0], ["richard", 0], ["burbage", 0], ["the", 0], ["leading", 0], ["tragedian", 0], ["of", 0], ["shakespeare", 0], ["s", 0], ["time", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.14285714285714285, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.07142857142857142, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.45454545454545453, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.22727272727272727, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.14285714285714285, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.07142857142857142, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.45454545454545453, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.22727272727272727, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Who were the hittites and what did they do?", "references": ["an Ancient Anatolian people who established an empire centered on Hattusa in north-central Anatolia around 1600 BC"], "id": "id2048"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id2048", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["who", 0], ["were", 0], ["the", 0], ["hittites", 0], ["and", 0], ["what", 0], ["did", 0], ["they", 0], ["do", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["an", 3], ["ancient", 3], ["anatolian", 3], ["people", 3], ["who", 3], ["established", 3], ["an", 0], ["empire", 0], ["centered", 0], ["on", 0], ["hattusa", 0], ["in", 0], ["north", 0], ["central", 0], ["anatolia", 0], ["around", 0], ["1600", 0], ["bc", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.3333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.33333333333333326, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.3333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.33333333333333326, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Who commissioned the first christmas card in 1943?", "references": ["Sir Henry Cole", "commissioned by Sir Henry Cole and illustrated by John Callcott Horsley in London on 1st May 1843"], "id": "id439"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id439", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["who", 0], ["commissioned", 0], ["the", 0], ["first", 0], ["christmas", 0], ["card", 0], ["in", 0], ["1943", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["sir", 0], ["henry", 0], ["cole", 0], ["commissioned", 3], ["by", 3], ["sir", 0], ["henry", 0], ["cole", 0], ["and", 0], ["illustrated", 0], ["by", 0], ["john", 0], ["callcott", 0], ["horsley", 0], ["in", 0], ["london", 0], ["on", 0], ["1st", 0], ["may", 0], ["1843", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.25, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.08333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.7, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.2333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.25, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.08333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.2333333333333333, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Where does see no evil speak no evil come from?", "references": ["a 17th-century carving over a door of the famous Tōshō-gū shrine in Nikkō, Japan", "The three wise monkeys (Japanese: 三猿, Hepburn: san'en or sanzaru, alternatively 三匹の猿 sanbiki no saru, literally \"three monkeys\"), sometimes called the three mystic apes"], "id": "id1882"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id1882", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["where", 0], ["does", 0], ["see", 0], ["no", 0], ["evil", 0], ["speak", 0], ["no", 0], ["evil", 0], ["come", 0], ["from", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["a", 1], ["17th", 1], ["century", 1], ["carving", 1], ["over", 0], ["a", 0], ["door", 0], ["of", 0], ["the", 0], ["famous", 0], ["tōshō", 0], ["gū", 0], ["shrine", 0], ["in", 0], ["nikkō", 0], ["japan", 0], ["the", 0], ["three", 0], ["wise", 0], ["monkeys", 0], ["japanese", 0], ["三猿", 0], ["hepburn", 0], ["san", 0], ["en", 0], ["or", 0], ["sanzaru", 0], ["alternatively", 0], ["三匹の猿", 0], ["sanbiki", 0], ["no", 0], ["saru", 0], ["literally", 0], ["three", 0], ["monkeys", 0], ["sometimes", 0], ["called", 0], ["the", 0], ["three", 0], ["mystic", 0], ["apes", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.13793103448275862, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.13793103448275862, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.3902439024390244, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.3902439024390244, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.13793103448275862, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.13793103448275862, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.3902439024390244, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.3902439024390244, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Where did the term liberal arts come from?", "references": ["the Roman Empire", "those subjects or skills that in classical antiquity were considered essential for a free person", "Latin: liberalis, \"worthy of a free person\"", "Latin: liberalis, free and ars, art or principled practice"], "id": "id845"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id845", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["where", 0], ["did", 0], ["the", 0], ["term", 0], ["liberal", 0], ["arts", 0], ["come", 0], ["from", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["the", 0], ["roman", 0], ["empire", 0], ["those", 6], ["subjects", 7], ["or", 7], ["skills", 0], ["that", 0], ["in", 0], ["classical", 0], ["antiquity", 0], ["were", 0], ["considered", 0], ["essential", 0], ["for", 0], ["a", 0], ["free", 0], ["person", 0], ["latin", 0], ["liberalis", 0], ["worthy", 0], ["of", 0], ["a", 0], ["free", 0], ["person", 0], ["latin", 0], ["liberalis", 0], ["free", 0], ["and", 0], ["ars", 0], ["art", 0], ["or", 0], ["principled", 0], ["practice", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.13636363636363635, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.020562770562770564, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.4411764705882353, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.07352941176470587, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.13636363636363635, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.020562770562770564, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.4411764705882353, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.07352941176470587, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Blood alcohol concentration means the parts of alcohol in the blood in relation to what?", "references": ["mass of alcohol per volume of blood or mass of alcohol per mass of blood", "volume of blood", "ethanol"], "id": "id3496"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3496", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["blood", 0], ["alcohol", 0], ["concentration", 0], ["means", 0], ["the", 0], ["parts", 0], ["of", 0], ["alcohol", 0], ["in", 0], ["the", 0], ["blood", 0], ["in", 0], ["relation", 0], ["to", 0], ["what", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["mass", 2], ["of", 2], ["alcohol", 2], ["per", 0], ["volume", 0], ["of", 0], ["blood", 0], ["or", 0], ["mass", 0], ["of", 0], ["alcohol", 0], ["per", 0], ["mass", 0], ["of", 0], ["blood", 0], ["volume", 0], ["of", 0], ["blood", 0], ["ethanol", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.42857142857142855, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.21428571428571427, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.7894736842105263, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.39473684210526316, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.42857142857142855, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.21428571428571427, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7894736842105263, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.39473684210526316, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What does the board of directors consist of?", "references": ["a recognized group of people who jointly oversee the activities of an organization"], "id": "id97"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id97", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["does", 0], ["the", 0], ["board", 0], ["of", 0], ["directors", 0], ["consist", 0], ["of", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["a", 1], ["recognized", 0], ["group", 0], ["of", 0], ["people", 0], ["who", 0], ["jointly", 0], ["oversee", 0], ["the", 0], ["activities", 0], ["of", 0], ["an", 0], ["organization", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "What is upstream project in oil and gas?", "references": ["searching for potential underground or underwater crude oil and natural gas fields, drilling exploratory wells, and subsequently drilling and operating the wells that recover and bring the crude oil or raw natural gas to the surface", "searching for potential underground or underwater crude oil and natural gas fields", "drilling exploratory wells", "drilling and operating the wells that recover and bring the crude oil or raw natural gas to the surface"], "id": "id3016"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3016", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["what", 0], ["is", 0], ["upstream", 0], ["project", 0], ["in", 0], ["oil", 0], ["and", 0], ["gas", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["searching", 4], ["for", 0], ["potential", 0], ["underground", 0], ["or", 0], ["underwater", 0], ["crude", 0], ["oil", 0], ["and", 0], ["natural", 0], ["gas", 0], ["fields", 0], ["drilling", 0], ["exploratory", 4], ["wells", 4], ["and", 3], ["subsequently", 3], ["drilling", 0], ["and", 0], ["operating", 0], ["the", 1], ["wells", 1], ["that", 1], ["recover", 1], ["and", 0], ["bring", 0], ["the", 0], ["crude", 0], ["oil", 0], ["or", 0], ["raw", 0], ["natural", 0], ["gas", 0], ["to", 0], ["the", 0], ["surface", 0], ["searching", 4], ["for", 0], ["potential", 0], ["underground", 0], ["or", 0], ["underwater", 0], ["crude", 0], ["oil", 0], ["and", 0], ["natural", 0], ["gas", 0], ["fields", 0], ["drilling", 0], ["exploratory", 0], ["wells", 0], ["drilling", 0], ["and", 0], ["operating", 0], ["the", 1], ["wells", 1], ["that", 1], ["recover", 1], ["and", 0], ["bring", 0], ["the", 0], ["crude", 0], ["oil", 0], ["or", 0], ["raw", 0], ["natural", 0], ["gas", 0], ["to", 0], ["the", 0], ["surface", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.2413793103448276, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.16666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.9285714285714286, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.7202380952380952, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.2413793103448276, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.16666666666666666, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.9285714285714286, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.7202380952380952, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Who was the movie citizen kane based on?", "references": ["based in part upon the American newspaper magnate William Randolph Hearst, Chicago tycoons Samuel Insull and Harold McCormick, and aspects of Welles's own life", "William Randolph Hearst"], "id": "id4136"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id4136", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["who", 0], ["was", 0], ["the", 0], ["movie", 0], ["citizen", 0], ["kane", 0], ["based", 0], ["on", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["based", 1], ["in", 1], ["part", 1], ["upon", 1], ["the", 3], ["american", 3], ["newspaper", 3], ["magnate", 3], ["william", 3], ["randolph", 1], ["hearst", 1], ["chicago", 1], ["tycoons", 1], ["samuel", 0], ["insull", 0], ["and", 0], ["harold", 0], ["mccormick", 0], ["and", 0], ["aspects", 0], ["of", 0], ["welles", 0], ["s", 0], ["own", 0], ["life", 0], ["william", 0], ["randolph", 0], ["hearst", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.8125, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.6041666666666665, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.8928571428571429, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.8928571428571429, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.8125, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.6041666666666665, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.8928571428571429, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.8928571428571429, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "When does a currency transaction report need to be filed?", "references": ["each deposit, withdrawal, exchange of currency, or other payment or transfer, by, through, or to the financial institution which involves a transaction in currency of more than $10,000", "a transaction in currency of more than $10,000", "for each deposit, withdrawal, exchange of currency, or other payment or transfer, by, through, or to the financial institution which involves a transaction in currency of more than $10,000"], "id": "id3028"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3028", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["when", 0], ["does", 0], ["a", 0], ["currency", 0], ["transaction", 0], ["report", 0], ["need", 0], ["to", 0], ["be", 0], ["filed", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["each", 12], ["deposit", 12], ["withdrawal", 12], ["exchange", 0], ["of", 0], ["currency", 0], ["or", 0], ["other", 0], ["payment", 0], ["or", 0], ["transfer", 0], ["by", 0], ["through", 0], ["or", 0], ["to", 0], ["the", 0], ["financial", 7], ["institution", 0], ["which", 0], ["involves", 0], ["a", 0], ["transaction", 0], ["in", 0], ["currency", 0], ["of", 0], ["more", 0], ["than", 0], ["10", 0], ["000", 0], ["a", 0], ["transaction", 0], ["in", 0], ["currency", 0], ["of", 0], ["more", 0], ["than", 0], ["10", 0], ["000", 0], ["for", 5], ["each", 12], ["deposit", 12], ["withdrawal", 12], ["exchange", 0], ["of", 0], ["currency", 0], ["or", 0], ["other", 0], ["payment", 0], ["or", 0], ["transfer", 0], ["by", 0], ["through", 0], ["or", 0], ["to", 0], ["the", 0], ["financial", 7], ["institution", 0], ["which", 0], ["involves", 0], ["a", 0], ["transaction", 0], ["in", 0], ["currency", 0], ["of", 0], ["more", 0], ["than", 0], ["10", 0], ["000", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.16071428571428573, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.01760204081632653, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.8382352941176471, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.12006302521008412, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.05357142857142857, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.008673469387755102, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.6176470588235294, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.10168067226890766, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "When is equilibrium reached in a chemical reaction?", "references": ["when the forward reaction proceeds at the same rate as the reverse reaction", "in which both reactants and products are present in concentrations which have no further tendency to change with time, so that there is no observable change in the properties of the system"], "id": "id4036"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id4036", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["when", 0], ["is", 0], ["equilibrium", 0], ["reached", 0], ["in", 0], ["a", 0], ["chemical", 0], ["reaction", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["when", 2], ["the", 0], ["forward", 0], ["reaction", 0], ["proceeds", 0], ["at", 0], ["the", 0], ["same", 0], ["rate", 0], ["as", 0], ["the", 0], ["reverse", 0], ["reaction", 0], ["in", 2], ["which", 2], ["both", 2], ["reactants", 2], ["and", 2], ["products", 2], ["are", 2], ["present", 0], ["in", 0], ["concentrations", 0], ["which", 0], ["have", 0], ["no", 0], ["further", 0], ["tendency", 0], ["to", 0], ["change", 0], ["with", 0], ["time", 0], ["so", 0], ["that", 0], ["there", 0], ["is", 0], ["no", 0], ["observable", 0], ["change", 0], ["in", 0], ["the", 0], ["properties", 0], ["of", 0], ["the", 0], ["system", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.24242424242424243, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.12121212121212122, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.7111111111111111, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.35555555555555557, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.24242424242424243, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.12121212121212122, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.7111111111111111, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.35555555555555557, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Who wrote i want to dance with somebody by whitney houston?", "references": ["written by George Merrill and Shannon Rubicam of the band Boy Meets Girl", "George Merrill", "Shannon Rubicam", "George Merrill and Shannon Rubicam", "George Merrill and Shannon Rubicam of the band Boy Meets Girl"], "id": "id1428"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id1428", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["who", 0], ["wrote", 0], ["i", 0], ["want", 0], ["to", 0], ["dance", 0], ["with", 0], ["somebody", 0], ["by", 0], ["whitney", 0], ["houston", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["written", 2], ["by", 0], ["george", 0], ["merrill", 0], ["and", 0], ["shannon", 0], ["rubicam", 0], ["of", 0], ["the", 0], ["band", 0], ["boy", 0], ["meets", 0], ["girl", 0], ["george", 0], ["merrill", 0], ["shannon", 0], ["rubicam", 0], ["george", 0], ["merrill", 0], ["and", 0], ["shannon", 0], ["rubicam", 0], ["george", 0], ["merrill", 0], ["and", 0], ["shannon", 0], ["rubicam", 0], ["of", 0], ["the", 0], ["band", 0], ["boy", 0], ["meets", 0], ["girl", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.047619047619047616, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.023809523809523808, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 0.3939393939393939, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.19696969696969696, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.047619047619047616, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.023809523809523808, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 0.3939393939393939, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.19696969696969696, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Why were the wars between rome and carthage called the punic wars?", "references": ["the Latin word Punicus (or Poenicus), meaning \"Carthaginian\", with reference to the Carthaginians' Phoenician ancestry"], "id": "id3352"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id3352", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["why", 0], ["were", 0], ["the", 0], ["wars", 0], ["between", 0], ["rome", 0], ["and", 0], ["carthage", 0], ["called", 0], ["the", 0], ["punic", 0], ["wars", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["the", 2], ["latin", 2], ["word", 2], ["punicus", 0], ["or", 0], ["poenicus", 0], ["meaning", 0], ["carthaginian", 0], ["with", 0], ["reference", 0], ["to", 0], ["the", 0], ["carthaginians", 0], ["phoenician", 0], ["ancestry", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.5, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
{"instance": {"input": "Where did the methodist church get its name?", "references": ["because of the way they used \"rule\" and \"method\" to go about their religious affairs"], "id": "id4042"}, "stats_key": {"light_scenario_key": {"scenario_spec": {"class_name": "helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario", "args": {"mode": "closedbook"}}, "split": "valid"}, "overlap_protocol_spec": {"n": 13}}, "instance_id": "id4042", "annotated_input_overlap": {"part": "input", "annotated_entry_overlap": [["where", 0], ["did", 0], ["the", 0], ["methodist", 0], ["church", 0], ["get", 0], ["its", 0], ["name", 0], ["", 0]], "metrics": []}, "annotated_ref_overlap": {"part": "references", "annotated_entry_overlap": [["because", 7], ["of", 7], ["the", 7], ["way", 0], ["they", 0], ["used", 0], ["rule", 0], ["and", 0], ["method", 0], ["to", 0], ["go", 0], ["about", 0], ["their", 0], ["religious", 0], ["affairs", 0]], "metrics": [{"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.14285714285714285, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": false}}}, {"metric_score": 0.14285714285714282, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 0, "weighting": true}}}, {"metric_score": 1, "metric_protocol_spec": {"partial_overlap_spec": 0, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.14285714285714285, "metric_protocol_spec": {"partial_overlap_spec": 1, "frequency_spec": {"filter_value": 10, "weighting": true}}}, {"metric_score": 1.0, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": false}}}, {"metric_score": 0.14285714285714282, "metric_protocol_spec": {"partial_overlap_spec": 2, "frequency_spec": {"filter_value": 10, "weighting": true}}}]}}
