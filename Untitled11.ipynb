{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044732e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cattrs\n",
    "import pandas as pd\n",
    "\n",
    "from data_overlap_spec import DataOverlapStats\n",
    "from light_scenario import GroupScenarioSpecs\n",
    "\n",
    "overlap_stats_path = 'output_stats_pile_all'\n",
    "overlap_stats_jsons = open(overlap_stats_path, \"r\").readlines()\n",
    "\n",
    "data_overlap_stats_list = []\n",
    "for overlap_stats_json in overlap_stats_jsons:\n",
    "    overlap_stats_dict = json.loads(overlap_stats_json)\n",
    "    data_overlap_stats_list.append(cattrs.structure(overlap_stats_dict, DataOverlapStats))\n",
    "\n",
    "scenario_spec_overlap_counts = dict()\n",
    "for data_overlap_stats in data_overlap_stats_list:\n",
    "    data_overlap_stats_key = data_overlap_stats.data_overlap_stats_key\n",
    "    light_scenario_key = data_overlap_stats_key.light_scenario_key\n",
    "    scenario_spec = light_scenario_key.scenario_spec\n",
    "    num_instances = data_overlap_stats.num_instances\n",
    "    n = data_overlap_stats_key.overlap_protocol_spec.n\n",
    "    num_overlapping_inputs = len(data_overlap_stats.instance_ids_with_overlapping_input)\n",
    "    num_overlapping_references = len(data_overlap_stats.instance_ids_with_overlapping_reference)\n",
    "    if n == 13:\n",
    "        scenario_spec_overlap_counts[scenario_spec] = (num_instances, num_overlapping_inputs, num_overlapping_references)\n",
    "\n",
    "group_scenario_specs_path = 'group_scenario_specs'\n",
    "group_scenario_specs_jsons = open(group_scenario_specs_path, \"r\").readlines()\n",
    "\n",
    "data_overlap_stats_list = []\n",
    "for group_scenario_specs_json in group_scenario_specs_jsons:\n",
    "    group_scenario_specs_dict = json.loads(group_scenario_specs_json)\n",
    "    data_overlap_stats_list.append(cattrs.structure(group_scenario_specs_dict, GroupScenarioSpecs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4571769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'sociology'}): (201,\n",
       "  0,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_psychology'}): (545,\n",
       "  7,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'number_theory', 'level': 1, 'use_official_examples': True, 'use_chain_of_thought': False}): (30,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'precalculus', 'level': 1, 'use_official_examples': False, 'use_chain_of_thought': True}): (57,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.synthetic_reasoning_scenario.SyntheticReasoningScenario', args={'mode': 'variable_substitution'}): (5000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'genre'}): (850,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.raft_scenario.RAFTScenario', args={'subset': 'overruling'}): (40,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'intermediate_algebra', 'level': 1, 'use_official_examples': False, 'use_chain_of_thought': True}): (52,\n",
       "  0,\n",
       "  2),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.bold_scenario.BOLDScenario', args={'subject': 'profession'}): (1064,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.lsat_qa_scenario.LSATScenario', args={'task': 'miscellaneous'}): (182,\n",
       "  3,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'precalculus', 'level': 5, 'use_official_examples': False, 'use_chain_of_thought': True}): (135,\n",
       "  2,\n",
       "  7),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'hk', 'category': 'W1'}): (50,\n",
       "  14,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.raft_scenario.RAFTScenario', args={'subset': 'neurips_impact_statement_risks'}): (40,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.synthetic_reasoning_scenario.SyntheticReasoningScenario', args={'mode': 'induction'}): (5000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'can', 'category': 'W1'}): (50,\n",
       "  17,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'ind', 'category': 'S2'}): (120,\n",
       "  18,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'usa', 'category': 'W2'}): (150,\n",
       "  40,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'category': 'S'}): (2371,\n",
       "  213,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'stock_exchange'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'sin', 'category': 'W2'}): (150,\n",
       "  18,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.lsat_qa_scenario.LSATScenario', args={'task': 'grouping'}): (5,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'public_relations'}): (110,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'defendant'}): (73,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'capital'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.blimp_scenario.BLiMPScenario', args={'phenomenon': 'filler_gap_dependency'}): (7000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'shares_border_with'}): (850,\n",
       "  0,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.lsat_qa_scenario.LSATScenario', args={'task': 'ordering'}): (20,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'philosophy'}): (311,\n",
       "  4,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'management'}): (103,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.summarization_scenario.SummarizationScenario', args={'dataset_name': 'xsum-sampled', 'sampling_min_length': 50, 'sampling_max_length': 150, 'doc_max_length': 512}): (11334,\n",
       "  4133,\n",
       "  870),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'precalculus', 'level': 2, 'use_official_examples': False, 'use_chain_of_thought': True}): (113,\n",
       "  0,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_mathematics'}): (100,\n",
       "  4,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.twitter_aae_scenario.TwitterAAEScenario', args={'demographic': 'white'}): (50000,\n",
       "  326,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.boolq_scenario.BoolQScenario', args={'only_contrast': False}): (3270,\n",
       "  1734,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'intermediate_algebra', 'level': 2, 'use_official_examples': False, 'use_chain_of_thought': True}): (128,\n",
       "  0,\n",
       "  3),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'NIH ExPorter'}): (189,\n",
       "  18,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'ea'}): (625,\n",
       "  63,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'professional_accounting'}): (282,\n",
       "  8,\n",
       "  5),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.copyright_scenario.CopyrightScenario', args={'datatag': 'prompt_num_line_5-min_lines_20.json'}): (1995,\n",
       "  1,\n",
       "  12),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'precalculus', 'level': 5, 'use_official_examples': True, 'use_chain_of_thought': False}): (135,\n",
       "  2,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'place_of_death'}): (850,\n",
       "  0,\n",
       "  14),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'therapeutic_area'}): (5,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'hk', 'category': 'W2'}): (150,\n",
       "  31,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'laws_applied'}): (18,\n",
       "  0,\n",
       "  2),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.imdb_scenario.IMDBScenario', args={'only_contrast': 'True'}): (356,\n",
       "  5,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'director'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'can', 'category': 'W2'}): (150,\n",
       "  19,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'number_theory', 'level': 2, 'use_official_examples': False, 'use_chain_of_thought': True}): (92,\n",
       "  0,\n",
       "  3),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'place_of_birth'}): (850,\n",
       "  0,\n",
       "  6),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'human_sexuality'}): (131,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.raft_scenario.RAFTScenario', args={'subset': 'systematic_review_inclusion'}): (40,\n",
       "  4,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'moral_disputes'}): (346,\n",
       "  3,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'medical_genetics'}): (100,\n",
       "  2,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.lex_glue_scenario.LexGLUEScenario', args={'subset': 'ecthr_b'}): (1000,\n",
       "  211,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'position_held'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'precalculus', 'level': 4, 'use_official_examples': False, 'use_chain_of_thought': True}): (114,\n",
       "  4,\n",
       "  4),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.commonsense_scenario.CommonSenseScenario', args={'dataset': 'commonsenseqa'}): (1221,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'subsidiary'}): (850,\n",
       "  0,\n",
       "  4),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'industry'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'geometry', 'level': 2, 'use_official_examples': True, 'use_chain_of_thought': False}): (82,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'head_of_government'}): (850,\n",
       "  0,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'counting_and_probability', 'level': 3, 'use_official_examples': False, 'use_chain_of_thought': True}): (100,\n",
       "  2,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.civil_comments_scenario.CivilCommentsScenario', args={'demographic': 'black'}): (4872,\n",
       "  277,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.entity_matching_scenario.EntityMatchingScenario', args={'dataset': 'Abt_Buy'}): (1916,\n",
       "  12,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.commonsense_scenario.CommonSenseScenario', args={'dataset': 'hellaswag'}): (10042,\n",
       "  306,\n",
       "  275),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'intermediate_algebra', 'level': 5, 'use_official_examples': False, 'use_chain_of_thought': True}): (280,\n",
       "  4,\n",
       "  5),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'genetic_association'}): (850,\n",
       "  0,\n",
       "  2),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.blimp_scenario.BLiMPScenario', args={'phenomenon': 'anaphor_agreement'}): (2000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.anthropic_hh_rlhf_scenario.AnthropicHHRLHFScenario', args={'subset': 'red_team'}): (38961,\n",
       "  37,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'solved_by'}): (63,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'ind', 'category': 'S1'}): (180,\n",
       "  4,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 18}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'country'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'algebra', 'level': 2, 'use_official_examples': True, 'use_chain_of_thought': False}): (201,\n",
       "  2,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'algebra', 'level': 4, 'use_official_examples': True, 'use_chain_of_thought': False}): (283,\n",
       "  3,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'intermediate_algebra', 'level': 4, 'use_official_examples': True, 'use_chain_of_thought': False}): (248,\n",
       "  6,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.newsqa_scenario.NewsQAScenario', args={}): (1273,\n",
       "  797,\n",
       "  34),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'StackExchange'}): (3029,\n",
       "  391,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'logical_fallacies'}): (163,\n",
       "  6,\n",
       "  3),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'gender': 'male'}): (694,\n",
       "  95,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.lex_glue_scenario.LexGLUEScenario', args={'subset': 'eurlex'}): (5000,\n",
       "  4990,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.open_assistant_scenario.OpenAssistantScenario', args={'language': 'en'}): (188,\n",
       "  1,\n",
       "  31),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.raft_scenario.RAFTScenario', args={'subset': 'banking_77'}): (40,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.raft_scenario.RAFTScenario', args={'subset': 'tai_safety_research'}): (40,\n",
       "  6,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'official_language'}): (850,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'us_foreign_policy'}): (100,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_biology'}): (144,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'astronomy'}): (152,\n",
       "  2,\n",
       "  3),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'nutrition'}): (306,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'econometrics'}): (114,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'ja', 'category': 'S2'}): (120,\n",
       "  17,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.legal_summarization_scenario.LegalSummarizationScenario', args={'dataset_name': 'EurLexSum', 'sampling_min_length': 400, 'sampling_max_length': 1600, 'doc_max_length': 2048}): (188,\n",
       "  188,\n",
       "  185),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.blimp_scenario.BLiMPScenario', args={'phenomenon': 'binding'}): (7000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.dyck_language_scenario.DyckLanguageScenario', args={'num_parenthesis_pairs': 4}): (500,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_geography'}): (198,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.grammar_scenario.GrammarScenario', args={'path': 'src/helm/benchmark/scenarios/best_chatgpt_prompts.yaml', 'tags': ''}): (320,\n",
       "  8,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.raft_scenario.RAFTScenario', args={'subset': 'terms_of_service'}): (40,\n",
       "  24,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'ind', 'category': 'W1'}): (50,\n",
       "  2,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.lsat_qa_scenario.LSATScenario', args={'task': 'all'}): (230,\n",
       "  5,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.blimp_scenario.BLiMPScenario', args={'phenomenon': 'argument_structure'}): (9000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'number_theory', 'level': 4, 'use_official_examples': True, 'use_chain_of_thought': False}): (142,\n",
       "  4,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_microeconomics'}): (238,\n",
       "  2,\n",
       "  3),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'prehistory'}): (324,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'precalculus', 'level': 4, 'use_official_examples': True, 'use_chain_of_thought': False}): (114,\n",
       "  4,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'number_theory', 'level': 3, 'use_official_examples': False, 'use_chain_of_thought': True}): (122,\n",
       "  2,\n",
       "  3),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'counting_and_probability', 'level': 1, 'use_official_examples': True, 'use_chain_of_thought': False}): (39,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'named_after'}): (850,\n",
       "  1,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'location_of_formation'}): (850,\n",
       "  0,\n",
       "  40),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'counting_and_probability', 'level': 5, 'use_official_examples': True, 'use_chain_of_thought': False}): (123,\n",
       "  3,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'FreeLaw'}): (496,\n",
       "  397,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'usa'}): (200,\n",
       "  59,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.entity_data_imputation_scenario.EntityDataImputationScenario', args={'dataset': 'Restaurant'}): (86,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'work_location'}): (850,\n",
       "  0,\n",
       "  3),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.imdb_scenario.IMDBScenario', args={'only_contrast': False}): (25356,\n",
       "  541,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'sin'}): (500,\n",
       "  41,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'country_of_origin'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'geometry', 'level': 3, 'use_official_examples': True, 'use_chain_of_thought': False}): (102,\n",
       "  3,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'office_held_by_head_of_state'}): (98,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'geometry', 'level': 5, 'use_official_examples': True, 'use_chain_of_thought': False}): (132,\n",
       "  5,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'prealgebra', 'level': 1, 'use_official_examples': True, 'use_chain_of_thought': False}): (86,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'USPTO Backgrounds'}): (1146,\n",
       "  277,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'occupation'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'professional_medicine'}): (272,\n",
       "  69,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'has_part'}): (850,\n",
       "  0,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'language_of_work_or_name'}): (850,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'precalculus', 'level': 3, 'use_official_examples': False, 'use_chain_of_thought': True}): (127,\n",
       "  0,\n",
       "  2),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'algebra', 'level': 1, 'use_official_examples': False, 'use_chain_of_thought': True}): (135,\n",
       "  4,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'algebra', 'level': 3, 'use_official_examples': False, 'use_chain_of_thought': True}): (261,\n",
       "  5,\n",
       "  2),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'instrument'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 4}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'phi'}): (500,\n",
       "  72,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'ind', 'category': 'W2'}): (150,\n",
       "  28,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'counting_and_probability', 'level': 2, 'use_official_examples': False, 'use_chain_of_thought': True}): (101,\n",
       "  3,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.bbq_scenario.BBQScenario', args={'subject': 'SES'}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 16}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'number_theory', 'level': 1, 'use_official_examples': False, 'use_chain_of_thought': True}): (30,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 1}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'location'}): (850,\n",
       "  0,\n",
       "  3),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'ja', 'category': 'S1'}): (180,\n",
       "  34,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'geometry', 'level': 3, 'use_official_examples': False, 'use_chain_of_thought': True}): (102,\n",
       "  3,\n",
       "  2),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'author'}): (850,\n",
       "  4,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'original_network'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_physics'}): (151,\n",
       "  5,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'statement_describes'}): (101,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'subclass_of'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'electrical_engineering'}): (145,\n",
       "  9,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.dyck_language_scenario.DyckLanguageScenario', args={'num_parenthesis_pairs': 2}): (500,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 6}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'native_language'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'number_theory', 'level': 5, 'use_official_examples': False, 'use_chain_of_thought': True}): (154,\n",
       "  2,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.twitter_aae_scenario.TwitterAAEScenario', args={'demographic': 'aa'}): (50000,\n",
       "  66,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.raft_scenario.RAFTScenario', args={'subset': 'ade_corpus_v2'}): (40,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'gender': 'female'}): (420,\n",
       "  58,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'developer'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'prealgebra', 'level': 3, 'use_official_examples': False, 'use_chain_of_thought': True}): (224,\n",
       "  4,\n",
       "  4),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'business_ethics'}): (100,\n",
       "  0,\n",
       "  2),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.synthetic_reasoning_natural_scenario.SRNScenario', args={'difficulty': 'hard'}): (5000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.quac_scenario.QuACScenario', args={}): (1000,\n",
       "  817,\n",
       "  222),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.raft_scenario.RAFTScenario', args={'subset': 'semiconductor_org_types'}): (40,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'member_of'}): (850,\n",
       "  0,\n",
       "  85),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.blimp_scenario.BLiMPScenario', args={'phenomenon': 'ellipsis'}): (2000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'ja', 'category': 'W1'}): (50,\n",
       "  3,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.vicuna_scenario.VicunaScenario', args={'category': 'all'}): (80,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'capital_of'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'continent'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'head_of_state'}): (534,\n",
       "  0,\n",
       "  4),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'central_bank'}): (114,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'operating_system'}): (850,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 11}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 9}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'ArXiv'}): (238,\n",
       "  83,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_statistics'}): (216,\n",
       "  1,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'programming_language'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'algebra', 'level': 5, 'use_official_examples': False, 'use_chain_of_thought': True}): (307,\n",
       "  4,\n",
       "  3),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_european_history'}): (165,\n",
       "  76,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.civil_comments_scenario.CivilCommentsScenario', args={'demographic': 'other_religions'}): (6544,\n",
       "  418,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'YoutubeSubtitles'}): (34,\n",
       "  11,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.synthetic_reasoning_natural_scenario.SRNScenario', args={'difficulty': 'easy'}): (5000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'professional_law'}): (1534,\n",
       "  38,\n",
       "  18),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'position_played_on_team'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'intermediate_algebra', 'level': 5, 'use_official_examples': True, 'use_chain_of_thought': False}): (280,\n",
       "  4,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'geometry', 'level': 4, 'use_official_examples': True, 'use_chain_of_thought': False}): (125,\n",
       "  3,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.raft_scenario.RAFTScenario', args={'subset': 'tweet_eval_hate'}): (40,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.civil_comments_scenario.CivilCommentsScenario', args={'demographic': 'all'}): (269038,\n",
       "  8457,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'ea', 'category': 'S2'}): (149,\n",
       "  14,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_computer_science'}): (100,\n",
       "  2,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'member_of_sports_team'}): (850,\n",
       "  0,\n",
       "  2),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'overrules'}): (5,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'geometry', 'level': 2, 'use_official_examples': False, 'use_chain_of_thought': True}): (82,\n",
       "  1,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'ja', 'category': 'W2'}): (150,\n",
       "  23,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'discoverer_or_inventor'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_mathematics'}): (270,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.bbq_scenario.BBQScenario', args={'subject': 'all'}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'Pile-CC'}): (5252,\n",
       "  2746,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'algebra', 'level': 2, 'use_official_examples': False, 'use_chain_of_thought': True}): (201,\n",
       "  2,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'HackerNews'}): (165,\n",
       "  30,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.copyright_scenario.CopyrightScenario', args={'datatag': 'prompt_num_line_10-min_lines_20.json'}): (1997,\n",
       "  5,\n",
       "  12),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'measured_physical_quantity'}): (373,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'intermediate_algebra', 'level': 4, 'use_official_examples': False, 'use_chain_of_thought': True}): (248,\n",
       "  6,\n",
       "  7),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.civil_comments_scenario.CivilCommentsScenario', args={'demographic': 'muslim'}): (2110,\n",
       "  116,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'PubMed Central'}): (584,\n",
       "  374,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'conceptual_physics'}): (235,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.bold_scenario.BOLDScenario', args={'subject': 'race'}): (3196,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'headquarters_location'}): (850,\n",
       "  0,\n",
       "  10),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.copyright_scenario.CopyrightScenario', args={'datatag': 'prompt_num_line_1-min_lines_20.json'}): (1986,\n",
       "  0,\n",
       "  12),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'languages_spoken_written_or_signed'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'twinned_administrative_body'}): (850,\n",
       "  0,\n",
       "  4),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario', args={'mode': 'openbook_longans'}): (2144,\n",
       "  1010,\n",
       "  100),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.real_toxicity_prompts_scenario.RealToxicityPromptsScenario', args={}): (99442,\n",
       "  6692,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.lex_glue_scenario.LexGLUEScenario', args={'subset': 'ledgar'}): (10000,\n",
       "  5637,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'influenced_by'}): (850,\n",
       "  0,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.civil_comments_scenario.CivilCommentsScenario', args={'demographic': 'male'}): (4765,\n",
       "  245,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'currency'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.civil_comments_scenario.CivilCommentsScenario', args={'demographic': 'LGBTQ'}): (1457,\n",
       "  59,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.lex_glue_scenario.LexGLUEScenario', args={'subset': 'scotus'}): (1400,\n",
       "  1398,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.code_scenario.CodeScenario', args={'dataset': 'apps'}): (5000,\n",
       "  1320,\n",
       "  464),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'number_theory', 'level': 2, 'use_official_examples': True, 'use_chain_of_thought': False}): (92,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'Gutenberg (PG-19)'}): (11,\n",
       "  10,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'algebra', 'level': 4, 'use_official_examples': False, 'use_chain_of_thought': True}): (283,\n",
       "  3,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_world_history'}): (237,\n",
       "  107,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'hk'}): (500,\n",
       "  67,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'prealgebra', 'level': 2, 'use_official_examples': True, 'use_chain_of_thought': False}): (177,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.gsm_scenario.GSM8KScenario', args={}): (1319,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'virology'}): (166,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.disinformation_scenario.DisinformationScenario', args={'capability': 'wedging', 'topic': 'covid'}): (11,\n",
       "  2,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.bold_scenario.BOLDScenario', args={'subject': 'all'}): (7201,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 14}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'marketing'}): (234,\n",
       "  5,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'geometry', 'level': 5, 'use_official_examples': False, 'use_chain_of_thought': True}): (132,\n",
       "  5,\n",
       "  4),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'applies_to_jurisdiction'}): (850,\n",
       "  3,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.entity_matching_scenario.EntityMatchingScenario', args={'dataset': 'Beer'}): (91,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'ea', 'category': 'S1'}): (122,\n",
       "  4,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'EuroParl'}): (16,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'employer'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'OpenSubtitles'}): (64,\n",
       "  44,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.blimp_scenario.BLiMPScenario', args={'phenomenon': 'quantifiers'}): (4000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'prealgebra', 'level': 4, 'use_official_examples': True, 'use_chain_of_thought': False}): (191,\n",
       "  4,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 19}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.dyck_language_scenario.DyckLanguageScenario', args={'num_parenthesis_pairs': 3}): (500,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'counting_and_probability', 'level': 3, 'use_official_examples': True, 'use_chain_of_thought': False}): (100,\n",
       "  2,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.lex_glue_scenario.LexGLUEScenario', args={'subset': 'unfair_tos'}): (1607,\n",
       "  1034,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.disinformation_scenario.DisinformationScenario', args={'capability': 'reiteration', 'topic': 'climate'}): (12,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'precalculus', 'level': 1, 'use_official_examples': True, 'use_chain_of_thought': False}): (57,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.msmarco_scenario.MSMARCOScenario', args={'track': 'regular', 'valid_topk': 30}): (6979,\n",
       "  2,\n",
       "  6845),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'ea', 'category': 'W1'}): (64,\n",
       "  19,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'plaintiff'}): (96,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 'all'}): (20000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 2}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'formal_logic'}): (126,\n",
       "  2,\n",
       "  2),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'Wikipedia (en)'}): (1737,\n",
       "  356,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'OpenWebText2'}): (3282,\n",
       "  1357,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'prealgebra', 'level': 5, 'use_official_examples': True, 'use_chain_of_thought': False}): (193,\n",
       "  9,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.bold_scenario.BOLDScenario', args={'subject': 'religious_ideology'}): (80,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'computer_security'}): (100,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'member_of_political_party'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'symptoms_and_signs'}): (566,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_biology'}): (310,\n",
       "  15,\n",
       "  7),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'prealgebra', 'level': 3, 'use_official_examples': True, 'use_chain_of_thought': False}): (224,\n",
       "  4,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'field_of_work'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'phi', 'category': 'S2'}): (120,\n",
       "  25,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'can'}): (500,\n",
       "  59,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.copyright_scenario.CopyrightScenario', args={'datatag': 'popular_books-prefix_length_125.json'}): (20,\n",
       "  17,\n",
       "  20),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'Books3'}): (19,\n",
       "  19,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'participating_team'}): (850,\n",
       "  0,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.blimp_scenario.BLiMPScenario', args={'phenomenon': 'irregular_forms'}): (2000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'phi', 'category': 'S1'}): (180,\n",
       "  14,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 7}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 17}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'machine_learning'}): (112,\n",
       "  1,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.legal_summarization_scenario.LegalSummarizationScenario', args={'dataset_name': 'MultiLexSum', 'sampling_min_length': 100, 'sampling_max_length': 400, 'doc_max_length': 1024}): (616,\n",
       "  524,\n",
       "  276),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_government_and_politics'}): (193,\n",
       "  4,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'sin', 'category': 'W1'}): (50,\n",
       "  5,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'repealed_by'}): (83,\n",
       "  0,\n",
       "  8),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'intermediate_algebra', 'level': 3, 'use_official_examples': False, 'use_chain_of_thought': True}): (195,\n",
       "  4,\n",
       "  3),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'algebra', 'level': 5, 'use_official_examples': True, 'use_chain_of_thought': False}): (307,\n",
       "  4,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 12}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_computer_science'}): (100,\n",
       "  0,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.raft_scenario.RAFTScenario', args={'subset': 'twitter_complaints'}): (40,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.blimp_scenario.BLiMPScenario', args={'phenomenon': 'control_raising'}): (5000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'ea', 'category': 'W2'}): (290,\n",
       "  26,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.truthful_qa_scenario.TruthfulQAScenario', args={'task': 'mc_single'}): (654,\n",
       "  1,\n",
       "  2),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'international_law'}): (121,\n",
       "  0,\n",
       "  7),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 15}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'moral_scenarios'}): (895,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'Github'}): (1797,\n",
       "  651,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'Ubuntu IRC'}): (22,\n",
       "  11,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'phi', 'category': 'W1'}): (50,\n",
       "  6,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'diplomatic_relation'}): (111,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'composer'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'ind'}): (500,\n",
       "  52,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'counting_and_probability', 'level': 2, 'use_official_examples': True, 'use_chain_of_thought': False}): (101,\n",
       "  3,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'professional_psychology'}): (612,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'located_in_the_administrative_territorial_entity'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'jurisprudence'}): (108,\n",
       "  3,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'geometry', 'level': 4, 'use_official_examples': False, 'use_chain_of_thought': True}): (125,\n",
       "  3,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'global_facts'}): (100,\n",
       "  5,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_macroeconomics'}): (390,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'part_of'}): (850,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'prealgebra', 'level': 5, 'use_official_examples': False, 'use_chain_of_thought': True}): (193,\n",
       "  9,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'owned_by'}): (850,\n",
       "  0,\n",
       "  3),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.bold_scenario.BOLDScenario', args={'subject': 'political_ideology'}): (498,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 5}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_us_history'}): (204,\n",
       "  161,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'movement'}): (850,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'office_held_by_head_of_government'}): (850,\n",
       "  0,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'intermediate_algebra', 'level': 1, 'use_official_examples': True, 'use_chain_of_thought': False}): (52,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'recommended_unit_of_measurement'}): (123,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'educated_at'}): (850,\n",
       "  0,\n",
       "  3),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_medicine'}): (173,\n",
       "  6,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'counting_and_probability', 'level': 4, 'use_official_examples': True, 'use_chain_of_thought': False}): (111,\n",
       "  5,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.civil_comments_scenario.CivilCommentsScenario', args={'demographic': 'christian'}): (4550,\n",
       "  312,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'category': 'W'}): (1954,\n",
       "  327,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'security_studies'}): (245,\n",
       "  0,\n",
       "  3),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.msmarco_scenario.MSMARCOScenario', args={'track': 'trec', 'valid_topk': 30}): (43,\n",
       "  0,\n",
       "  43),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.copyright_scenario.CopyrightScenario', args={'datatag': 'n_books_1000-extractions_per_book_1-prefix_length_125'}): (979,\n",
       "  58,\n",
       "  181),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'world_religions'}): (171,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.blimp_scenario.BLiMPScenario', args={'phenomenon': 'determiner_noun_agreement'}): (8000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.med_qa_scenario.MedQAScenario', args={}): (1273,\n",
       "  197,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'human_aging'}): (223,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'anatomy'}): (135,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'phi', 'category': 'W2'}): (150,\n",
       "  27,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'creator'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'elementary_mathematics'}): (378,\n",
       "  9,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.code_scenario.CodeScenario', args={'dataset': 'humaneval'}): (164,\n",
       "  3,\n",
       "  4),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.koala_scenario.KoalaScenario', args={}): (180,\n",
       "  4,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'number_theory', 'level': 5, 'use_official_examples': True, 'use_chain_of_thought': False}): (154,\n",
       "  2,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'counting_and_probability', 'level': 1, 'use_official_examples': False, 'use_chain_of_thought': True}): (39,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.entity_data_imputation_scenario.EntityDataImputationScenario', args={'dataset': 'Buy'}): (65,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'precalculus', 'level': 2, 'use_official_examples': True, 'use_chain_of_thought': False}): (113,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.anthropic_hh_rlhf_scenario.AnthropicHHRLHFScenario', args={'subset': 'hh'}): (8552,\n",
       "  29,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'intermediate_algebra', 'level': 2, 'use_official_examples': True, 'use_chain_of_thought': False}): (128,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'manufacturer'}): (850,\n",
       "  1,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.disinformation_scenario.DisinformationScenario', args={'capability': 'reiteration', 'topic': 'covid'}): (31,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'drug_or_therapy_used_for_treatment'}): (813,\n",
       "  0,\n",
       "  236),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.lex_glue_scenario.LexGLUEScenario', args={'subset': 'case_hold'}): (3600,\n",
       "  1483,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'prealgebra', 'level': 2, 'use_official_examples': False, 'use_chain_of_thought': True}): (177,\n",
       "  0,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'country_of_citizenship'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.commonsense_scenario.CommonSenseScenario', args={'dataset': 'openbookqa'}): (500,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'number_theory', 'level': 4, 'use_official_examples': False, 'use_chain_of_thought': True}): (142,\n",
       "  4,\n",
       "  3),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_physics'}): (102,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'miscellaneous'}): (783,\n",
       "  7,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'PubMed Abstracts'}): (2990,\n",
       "  152,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'location_of_discovery'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.civil_comments_scenario.CivilCommentsScenario', args={'demographic': 'female'}): (16449,\n",
       "  654,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 20}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'ja'}): (500,\n",
       "  77,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'basic_form_of_government'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'abstract_algebra'}): (100,\n",
       "  2,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'BookCorpus2'}): (28,\n",
       "  25,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.natural_qa_scenario.NaturalQAScenario', args={'mode': 'closedbook'}): (2144,\n",
       "  1,\n",
       "  100),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'intermediate_algebra', 'level': 3, 'use_official_examples': True, 'use_chain_of_thought': False}): (195,\n",
       "  4,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 3}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.summarization_scenario.SummarizationScenario', args={'dataset_name': 'cnn-dm', 'sampling_min_length': 50, 'sampling_max_length': 150, 'doc_max_length': 512}): (11490,\n",
       "  3905,\n",
       "  323),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'prealgebra', 'level': 4, 'use_official_examples': False, 'use_chain_of_thought': True}): (191,\n",
       "  4,\n",
       "  4),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.raft_scenario.RAFTScenario', args={'subset': 'one_stop_english'}): (40,\n",
       "  26,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.synthetic_reasoning_scenario.SyntheticReasoningScenario', args={'mode': 'pattern_match'}): (5000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikitext_103_scenario.Wikitext103Scenario', args={}): (60,\n",
       "  43,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'Enron Emails'}): (102,\n",
       "  5,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'geometry', 'level': 1, 'use_official_examples': True, 'use_chain_of_thought': False}): (38,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'counting_and_probability', 'level': 5, 'use_official_examples': False, 'use_chain_of_thought': True}): (123,\n",
       "  3,\n",
       "  3),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'record_label'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'clinical_knowledge'}): (265,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.blimp_scenario.BLiMPScenario', args={'phenomenon': 'subject_verb_agreement'}): (6000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.blimp_scenario.BLiMPScenario', args={'phenomenon': 'island_effects'}): (8000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 8}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.blimp_scenario.BLiMPScenario', args={'phenomenon': 'npi_licensing'}): (7000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'sin', 'category': 'S2'}): (120,\n",
       "  11,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'award_received'}): (850,\n",
       "  0,\n",
       "  4),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 13}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'instance_of'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'college_chemistry'}): (100,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'geometry', 'level': 1, 'use_official_examples': False, 'use_chain_of_thought': True}): (38,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'sin', 'category': 'S1'}): (180,\n",
       "  7,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'number_theory', 'level': 3, 'use_official_examples': True, 'use_chain_of_thought': False}): (122,\n",
       "  2,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'religion'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.self_instruct_scenario.SelfInstructScenario', args={}): (252,\n",
       "  12,\n",
       "  19),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.boolq_scenario.BoolQScenario', args={'only_contrast': 'True'}): (64,\n",
       "  35,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.civil_comments_scenario.CivilCommentsScenario', args={'demographic': 'white'}): (7969,\n",
       "  386,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.legal_support_scenario.LegalSupportScenario', args={}): (3047,\n",
       "  267,\n",
       "  632),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.lsat_qa_scenario.LSATScenario', args={'task': 'assignment'}): (22,\n",
       "  2,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.babi_qa_scenario.BabiQAScenario', args={'task': 10}): (1000,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'original_language_of_film_or_TV_show'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'PhilPapers'}): (68,\n",
       "  48,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'usa', 'category': 'W1'}): (50,\n",
       "  19,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'hk', 'category': 'S2'}): (120,\n",
       "  10,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'medical_condition_treated'}): (850,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.wikifact_scenario.WIKIFactScenario', args={'subject': 'majority_opinion_by'}): (850,\n",
       "  2,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.lex_glue_scenario.LexGLUEScenario', args={'subset': 'ecthr_a'}): (1000,\n",
       "  211,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.bold_scenario.BOLDScenario', args={'subject': 'gender'}): (2363,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'can', 'category': 'S2'}): (120,\n",
       "  13,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.the_pile_scenario.ThePileScenario', args={'subset': 'DM Mathematics'}): (186,\n",
       "  3,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'high_school_chemistry'}): (203,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'hk', 'category': 'S1'}): (180,\n",
       "  12,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.narrativeqa_scenario.NarrativeQAScenario', args={}): (355,\n",
       "  175,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'precalculus', 'level': 3, 'use_official_examples': True, 'use_chain_of_thought': False}): (127,\n",
       "  0,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'prealgebra', 'level': 1, 'use_official_examples': False, 'use_chain_of_thought': True}): (86,\n",
       "  0,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.legal_summarization_scenario.LegalSummarizationScenario', args={'dataset_name': 'BillSum', 'sampling_min_length': 200, 'sampling_max_length': 800, 'doc_max_length': 2048}): (3269,\n",
       "  3247,\n",
       "  1319),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'algebra', 'level': 1, 'use_official_examples': True, 'use_chain_of_thought': False}): (135,\n",
       "  4,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'algebra', 'level': 3, 'use_official_examples': True, 'use_chain_of_thought': False}): (261,\n",
       "  5,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.ice_scenario.ICEScenario', args={'subset': 'can', 'category': 'S1'}): (180,\n",
       "  10,\n",
       "  0),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.math_scenario.MATHScenario', args={'subject': 'counting_and_probability', 'level': 4, 'use_official_examples': False, 'use_chain_of_thought': True}): (111,\n",
       "  5,\n",
       "  1),\n",
       " ScenarioSpec(class_name='helm.benchmark.scenarios.entity_matching_scenario.EntityMatchingScenario', args={'dataset': 'Dirty_iTunes_Amazon'}): (109,\n",
       "  44,\n",
       "  0)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario_spec_overlap_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf456a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
